{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/magnetclip/nlp/blob/main/codeitnlp.ipynb","timestamp":1679026934887}],"collapsed_sections":["G3mvjU_vfRJu","YhJ_wUjVfrX1","kmbBPYiKDRyp","ZGdJ9jPJkKe8","prK6Dc5RHOo6"],"mount_file_id":"1BnlcZ-HHZuPgzNzZlc1jFJK6_wYjGWdz","authorship_tag":"ABX9TyP7/fT9kaY4ZWCcebayk/Xf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 자연어전처리과정\n","\n","## 단어 단위의 전처리\n","* 단어 토큰화(word tokenization); 띄어쓰기, 문장기호(', , 등)을 기준으로 단어 리스트화\n","* 정제(cleaning); 코퍼스(분석에 활용하기 위한 자연어 데이터 (말뭉치))에서 의미 없거나 목적에 적합하지 않은 단어를 제거 (예. 빈도수 2 이하인 단어, 길이가 2 이하인 단어 등)\n","* 불용어(stopwords) 정의; 의미 없거나 목적에서 벗어나는 단어(목적에 맞게 정의하여)를 제거 (예. do, then, what, she, am, are ...)\n","* 정규화(normalization); 형태는 다르지만 같은 의미로 사용되는 단어를 하나로 통일 (예. US, USA, U.S., United States of America ...)\n","* 어간추출(stemming); 특정단어의 핵심이 되는 부분(어간)을 찾아 정규화(예. alize->al, ational->ate, ate->제거 ment->제거 등). porter stemmer, lancaster stemmer 등이 있음. 단, 단순히 어미만 잘라내는 방식으로, activate->activ 가 사전에 없어, 섬세하지 못함\n","\n","\n","## 문장 단위의 전처리\n","* 품사 태깅 등, 문장안에서 단어가 사용된 위치에 따라 품사가 달라지는 경우, 단어 단위가 아니라 문장 간의 구분이 된 상태에서 단어의 품사를 정해야 함\n","* 문장 토큰화(sentence tokenization); 코퍼스를 문장 단위로 토큰화. 마침표를 기준으로 토큰화. dr. mr. 의 마침표는 문장으로 인식하지 않아야 함\n","* 품사 태깅(POS; part of speech tagging); 문장 안에서의 단어의 품사를 태깅\n","    * wordnet; 거대한 영어 어휘 데이터베이스. n(wn.NOUN) 명사, a(wn.ADJ) 형용사, r(wn.ADV) 부사, v(wn.VERB) 동사 태그가 존재\n","* 표제어추출(Lemmatization); 단어의 사전적 어원 태깅 (예. happyiest->happy, am, are, is->be)을 통해 단어를 정규화\n",""],"metadata":{"id":"pZxLZU9MLh3Q"}},{"cell_type":"code","source":["# library import and function definition. preprocess.py\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')  # needed for punctuation and acronym such as Mr. Dr. ...\n","nltk.download('averaged_perceptron_tagger')  # for tagger\n","from collections import Counter\n","from nltk.stem import PorterStemmer\n","from nltk.tag import pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet as wn\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.corpus import stopwords  # 코퍼스의 종류에 상관없이 많이 사용되는 불용어 179개를 제공\n","nltk.download('stopwords')\n","nltk.download('sentiwordnet')\n","from nltk.corpus import sentiwordnet as swn\n","#\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","\n","stopwords_set = set(stopwords.words('english'))\n","\n","def clean_by_freq(tokenized_words, cut_off_count):\n","    vocab = Counter(tokenized_words)\n","\n","    uncommon_words = [key for key, value in vocab.items() if value <= cut_off_count]\n","    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n","\n","    return cleaned_words\n","\n","def clean_by_len(tokenized_words, cut_off_length):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        if len(word) > cut_off_length:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","def clean_by_stopwords(tokenized_words, stopwords_set):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        # 여기에 코드를 작성하세요\n","        if word not in stopwords_set:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","# 포터 스테머 어간 추출 함수\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        # porter_stemmed_words.append(porter_stemmer.stem(word))\n","        stem = porter_stemmer.stem(word)\n","        porter_stemmed_words.append(stem)\n","\n","    return porter_stemmed_words\n","\n","# 품사 태깅 함수\n","def pos_tagger(tokenized_sents):\n","    pos_tagged_words = []\n","    for sentence in tokenized_sents:\n","        # word tokenize\n","        tokenized_words = word_tokenize(sentence)\n","\n","        # pos\n","        pos_tagged = pos_tag(tokenized_words)\n","        pos_tagged_words.extend(pos_tagged)\n","    return pos_tagged_words\n","\n","# 품사 태깅 변환\n","def penn_to_wn(tag):\n","    if tag.startswith('J'):\n","        return wn.ADJ\n","    elif tag.startswith('N'):\n","        return wn.NOUN\n","    elif tag.startswith('R'):\n","        return wn.ADV\n","    elif tag.startswith('V'):\n","        return wn.VERB\n","    else:\n","        return\n","\n","# 표제어(lemmatization)\n","def word_lemmatizer(pos_tagged_words):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = []\n","    for word, tag in pos_tagged_words:\n","        wn_tag = penn_to_wn(tag)\n","        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n","        else:\n","            lemmatized_words.append(word)\n","    return lemmatized_words\n","\n","def combine(sentence):\n","    return ' '.join(sentence)\n","\n","def idx_encoder(tokens, word_to_idx):\n","    encoded_idx = []\n","    for token in tokens:\n","        idx = word_to_idx[token]\n","        encoded_idx.append(idx)\n","    return encoded_idx\n","\n","def get_sentiment_score(word, pos):\n","    # 단어와 품사 태그를 기반으로 Synsets 구하기\n","    word_sentisynsets = list(swn.senti_synsets(word, pos))\n","\n","    # Synsets의 첫 번째 요소의 이름으로 단일 SentiSynset 구하기\n","\n","    # SentiSynset의 긍정 지수, 부정 지수 구하기\n","    pos_score = word_sentisynsets[0].pos_score()\n","    neg_score = word_sentisynsets[0].neg_score()\n","\n","    # 긍정 지수 - 부정 지수로 감성 지수 값 계산해 반환하기\n","    sentiment_score = pos_score-neg_score\n","\n","    return sentiment_score\n","\n","def swn_polarity(pos_tagged_words):  # 감성 지수를 구하는 코드\n","    senti_score = 0\n","\n","    for word, tag in pos_tagged_words:\n","        # PennTreeBank 기준 품사를 WordNet 기준 품사로 변경\n","        wn_tag = penn_to_wn(tag)\n","        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","            continue\n","\n","        # Synset 확인, 어휘 사전에 없을 경우에는 스킵\n","        if not wn.synsets(word, wn_tag):\n","            continue\n","        else:\n","            synsets = wn.synsets(word, wn_tag)\n","\n","        # SentiSynset 확인\n","        synset = synsets[0]\n","        swn_synset = swn.senti_synset(synset.name())\n","\n","        # 감성 지수 계산. pos 감성에서 neg 감성을 빼고, 그 총합을 senti_score로 집계\n","        word_senti_score = (swn_synset.pos_score() - swn_synset.neg_score())\n","        senti_score += word_senti_score\n","\n","    return senti_score\n","\n","def vader_sentiment(text):\n","    analyzer = SentimentIntensityAnalyzer()\n","\n","    senti_score = analyzer.polarity_scores(text)['compound']\n","\n","    return senti_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7WT2tOhcfc1C","executionInfo":{"status":"ok","timestamp":1701669474621,"user_tz":-540,"elapsed":3427,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"1cbac766-e1c6-4fa2-8871-8d5c26d46e87"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n","[nltk_data]   Package sentiwordnet is already up-to-date!\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## chapter 2. 단어 단위 전처리"],"metadata":{"id":"G3mvjU_vfRJu"}},{"cell_type":"code","execution_count":26,"metadata":{"id":"hf1SV5ZKzrHG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700815622800,"user_tz":-540,"elapsed":13,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"3e96e400-ecec-40d4-e5e2-923030c411a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Although', 'it', \"'s\", 'not', 'a', 'happily-ever-after', 'ending', ',', 'it', 'is', 'very', 'realistic', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["# chapter 2, lesson 2 단어 토큰화\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","text = \"Although it's not a happily-ever-after ending, it is very realistic.\"\n","\n","# 단어 토큰화  https://www.nltk.org/api/nltk.tokenize.html\n","tokenized_words = word_tokenize(text)\n","\n","print(tokenized_words)"]},{"cell_type":"code","source":["# chapter 2, lesson 3 단어 토큰화 실습\n","import nltk\n","#from text import TEXT\n","from nltk.tokenize import word_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","\n","# 단어 토큰화\n","tokenized_words = word_tokenize(corpus)\n","\n","print(tokenized_words)"],"metadata":{"id":"icl3F9g62AkZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700815622801,"user_tz":-540,"elapsed":13,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"93dd901b-df75-470a-b9dc-419f466857b8"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'and\", 'what', 'is', 'the', 'use', 'of', 'a', 'book', ',', \"'\", 'thought', 'Alice', \"'without\", 'pictures', 'or', 'conversation', '?', \"'\", 'So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', ')', ',', 'whether', 'the', 'pleasure', 'of', 'making', 'a', 'daisy-chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.', 'There', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', ';', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'Rabbit', 'say', 'to', 'itself', ',', \"'Oh\", 'dear', '!', 'Oh', 'dear', '!', 'I', 'shall', 'be', 'late', '!', \"'\", '(', 'when', 'she', 'thought', 'it', 'over', 'afterwards', ',', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'have', 'wondered', 'at', 'this', ',', 'but', 'at', 'the', 'time', 'it', 'all', 'seemed', 'quite', 'natural', ')', ';', 'but', 'when', 'the', 'Rabbit', 'actually', 'took', 'a', 'watch', 'out', 'of', 'its', 'waistcoat-pocket', ',', 'and', 'looked', 'at', 'it', ',', 'and', 'then', 'hurried', 'on', ',', 'Alice', 'started', 'to', 'her', 'feet', ',', 'for', 'it', 'flashed', 'across', 'her', 'mind', 'that', 'she', 'had', 'never', 'before', 'seen', 'a', 'rabbit', 'with', 'either', 'a', 'waistcoat-pocket', ',', 'or', 'a', 'watch', 'to', 'take', 'out', 'of', 'it', ',', 'and', 'burning', 'with', 'curiosity', ',', 'she', 'ran', 'across', 'the', 'field', 'after', 'it', ',', 'and', 'fortunately', 'was', 'just', 'in', 'time', 'to', 'see', 'it', 'pop', 'down', 'a', 'large', 'rabbit-hole', 'under', 'the', 'hedge', '.', 'In', 'another', 'moment', 'down', 'went', 'Alice', 'after', 'it', ',', 'never', 'once', 'considering', 'how', 'in', 'the', 'world', 'she', 'was', 'to', 'get', 'out', 'again', '.', 'The', 'rabbit-hole', 'went', 'straight', 'on', 'like', 'a', 'tunnel', 'for', 'some', 'way', ',', 'and', 'then', 'dipped', 'suddenly', 'down', ',', 'so', 'suddenly', 'that', 'Alice', 'had', 'not', 'a', 'moment', 'to', 'think', 'about', 'stopping', 'herself', 'before', 'she', 'found', 'herself', 'falling', 'down', 'a', 'very', 'deep', 'well', '.', 'Either', 'the', 'well', 'was', 'very', 'deep', ',', 'or', 'she', 'fell', 'very', 'slowly', ',', 'for', 'she', 'had', 'plenty', 'of', 'time', 'as', 'she', 'went', 'down', 'to', 'look', 'about', 'her', 'and', 'to', 'wonder', 'what', 'was', 'going', 'to', 'happen', 'next', '.', 'First', ',', 'she', 'tried', 'to', 'look', 'down', 'and', 'make', 'out', 'what', 'she', 'was', 'coming', 'to', ',', 'but', 'it', 'was', 'too', 'dark', 'to', 'see', 'anything', ';', 'then', 'she', 'looked', 'at', 'the', 'sides', 'of', 'the', 'well', ',', 'and', 'noticed', 'that', 'they', 'were', 'filled', 'with', 'cupboards', 'and', 'book-shelves', ';', 'here', 'and', 'there', 'she', 'saw', 'maps', 'and', 'pictures', 'hung', 'upon', 'pegs', '.', 'She', 'took', 'down', 'a', 'jar', 'from', 'one', 'of', 'the', 'shelves', 'as', 'she', 'passed', ';', 'it', 'was', 'labelled', \"'ORANGE\", 'MARMALADE', \"'\", ',', 'but', 'to', 'her', 'great', 'disappointment', 'it', 'was', 'empty', ':', 'she', 'did', 'not', 'like', 'to', 'drop', 'the', 'jar', 'for', 'fear', 'of', 'killing', 'somebody', ',', 'so', 'managed', 'to', 'put', 'it', 'into', 'one', 'of', 'the', 'cupboards', 'as', 'she', 'fell', 'past', 'it', '.']\n"]}]},{"cell_type":"code","source":["# chapter 2, lesson 4 정제(cleaning); cleaning by word counts\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","\n","corpus = TEXT\n","\n","# token list\n","tokenized_words = word_tokenize(corpus)\n","print(len(tokenized_words))\n","\n","# token count\n","vocab = Counter(tokenized_words)  # {words: counter}\n","print('vocab is ', vocab)\n","\n","uncommon_words = [key for key, value in vocab.items() if value <= 2]\n","print('uncommon_words are ', uncommon_words)\n","print('frequency <= 2;', len(uncommon_words))\n","\n","cleaned_by_freq = [word for word in tokenized_words if word not in uncommon_words]\n","print('common_words are ', cleaned_by_freq)\n","print('frequency >= 3; ', len(cleaned_by_freq))"],"metadata":{"id":"Zp32mCwr246U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700815622801,"user_tz":-540,"elapsed":11,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"4db3224e-9a3c-49e6-9f04-cb4b56e3556a"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["579\n","vocab is  Counter({'the': 30, '.': 28, ',': 21, 'of': 15, 'and': 14, 'to': 13, 'a': 12, 'military': 12, 'in': 12, 'people': 9, 'on': 9, 'are': 9, 'for': 7, 'this': 7, 'that': 6, 'I': 5, 'The': 5, 'you': 5, 'not': 4, 'or': 4, 'about': 4, 'US': 4, 'at': 4, 'every': 4, 'it': 4, 'make': 4, 'was': 4, 'movie': 3, 'be': 3, 'who': 3, 'they': 3, 'Abu-Gharib': 3, 'makes': 3, 'number': 3, 'million': 3, 'with': 3, 'total': 3, 'would': 3, 'an': 3, 'there': 3, 'days': 3, 'hour': 3, 'minimum': 3, 'get': 3, 'comments': 2, ')': 2, 'know': 2, 'nothing': 2, 'base': 2, 'state': 2, 'world': 2, 'time': 2, ':': 2, '2.3': 2, 'indicted': 2, 'than': 2, 'That': 2, \"'s\": 2, 'but': 2, 'reality': 2, 'is': 2, 'first': 2, 'aid': 2, 'When': 2, 'their': 2, 'Within': 2, 'hours': 2, 'food': 2, 'months': 2, 'But': 2, 'website': 2, 'men': 2, 'women': 2, 'so': 2, 'personal': 2, 'gain': 2, 'under': 2, '40': 2, 'work': 2, 'week': 2, 'much': 2, 'ranks': 2, 'degrees': 2, 'After': 1, 'reading': 1, 'am': 1, 'sure': 1, 'whether': 1, 'should': 1, 'angry': 1, 'sad': 1, 'sickened': 1, 'Seeing': 1, 'typical': 1, 'absolutely': 1, 'b': 1, 'everything': 1, 'think': 1, 'movies': 1, 'like': 1, 'CNN': 1, 'reports': 1, 'me': 1, 'wonder': 1, 'intellectual': 1, 'stimulation': 1, 'At': 1, 'type': 1, '1.4': 1, 'Active': 1, 'Duty': 1, 'another': 1, 'almost': 1, '900,000': 1, 'Guard': 1, 'Reserves': 1, 'roughly': 1, 'abuses': 1, 'Currently': 1, 'less': 1, '20': 1, '.00083': 1, '%': 1, 'Even': 1, 'if': 1, 'indict': 1, 'single': 1, 'member': 1, 'ever': 1, 'stepped': 1, 'come': 1, 'close': 1, 'making': 1, 'whole': 1, 'flaws': 1, 'take': 1, 'YEARS': 1, 'cover': 1, 'understand': 1, 'supposed': 1, 'sarcastic': 1, 'writer': 1, 'director': 1, 'trying': 1, 'commentary': 1, 'without': 1, 'enemy': 1, 'fight': 1, 'In': 1, 'has': 1, 'been': 1, 'its': 1, 'busiest': 1, 'when': 1, 'conflicts': 1, 'going': 1, 'called': 1, 'disaster': 1, 'relief': 1, 'humanitarian': 1, 'missions': 1, 'tsunami': 1, 'hit': 1, 'Indonesia': 1, 'devestating': 1, 'region': 1, 'scene': 1, 'chaos': 1, 'situation': 1, 'overwhelmed': 1, 'local': 1, 'governments': 1, 'leadership': 1, 'looked': 1, 'same': 1, 'mocks': 1, 'said': 1, 'happen': 1, 'reaching': 1, 'isolated': 1, 'villages': 1, 'airfields': 1, 'were': 1, 'built': 1, 'cargo': 1, 'aircraft': 1, 'started': 1, 'landing': 1, 'distribution': 1, 'system': 1, 'up': 1, 'running': 1, 'Hours': 1, 'weeks': 1, 'Yes': 1, 'unscrupulous': 1, 'then': 1, 'walk': 1, 'life': 1, 'occupation': 1, 'see': 1, 'decide': 1, 'all': 1, 'criminal': 1, 'minds': 1, 'thoughts': 1, 'destruction': 1, 'mayhem': 1, 'absolute': 1, 'disservice': 1, 'things': 1, 'do': 1, 'day': 1, 'One': 1, 'person': 1, 'even': 1, 'went': 1, 'far': 1, 'as': 1, 'say': 1, 'members': 1, 'Wow': 1, '!': 1, 'Entry': 1, 'level': 1, 'personnel': 1, 'just': 1, '$': 1, '8.00': 1, 'assuming': 1, 'Of': 1, 'course': 1, 'many': 1, 'more': 1, 'those': 1, 'harm': 1, 'way': 1, 'typically': 1, 'put': 1, '16-18': 1, 'end': 1, 'pay': 1, 'well': 1, 'wage': 1, 'So': 1, 'beg': 1, 'please': 1, 'yourself': 1, 'familiar': 1, 'around': 1, 'Go': 1, 'nearby': 1, 'visitor': 1, 'pass': 1, 'meet': 1, 'some': 1, 'quick': 1, 'disparage': 1, 'You': 1, 'surprised': 1, 'no': 1, 'longer': 1, 'accepts': 1, 'lieu': 1, 'prison': 1, 'They': 1, 'require': 1, 'GED': 1, 'prefer': 1, 'high': 1, 'school': 1, 'diploma': 1, 'middle': 1, 'expected': 1, 'undergraduate': 1, 'upper': 1, 'encouraged': 1, 'advanced': 1})\n","uncommon_words are  ['After', 'reading', 'comments', 'am', 'sure', 'whether', 'should', 'angry', 'sad', 'sickened', 'Seeing', 'typical', ')', 'know', 'absolutely', 'nothing', 'b', 'base', 'everything', 'think', 'movies', 'like', 'CNN', 'reports', 'me', 'wonder', 'state', 'intellectual', 'stimulation', 'world', 'At', 'time', 'type', ':', '1.4', 'Active', 'Duty', 'another', 'almost', '900,000', 'Guard', 'Reserves', 'roughly', '2.3', 'indicted', 'abuses', 'Currently', 'less', 'than', '20', 'That', '.00083', '%', 'Even', 'if', 'indict', 'single', 'member', 'ever', 'stepped', 'come', 'close', 'making', 'whole', 'flaws', 'take', 'YEARS', 'cover', 'understand', \"'s\", 'supposed', 'sarcastic', 'but', 'reality', 'writer', 'director', 'trying', 'commentary', 'without', 'enemy', 'fight', 'In', 'has', 'been', 'its', 'busiest', 'when', 'conflicts', 'going', 'is', 'first', 'called', 'disaster', 'relief', 'humanitarian', 'aid', 'missions', 'When', 'tsunami', 'hit', 'Indonesia', 'devestating', 'region', 'scene', 'chaos', 'situation', 'overwhelmed', 'local', 'governments', 'leadership', 'looked', 'their', 'same', 'mocks', 'said', 'happen', 'Within', 'hours', 'food', 'reaching', 'isolated', 'villages', 'airfields', 'were', 'built', 'cargo', 'aircraft', 'started', 'landing', 'distribution', 'system', 'up', 'running', 'Hours', 'weeks', 'months', 'Yes', 'unscrupulous', 'But', 'then', 'walk', 'life', 'occupation', 'see', 'website', 'decide', 'men', 'women', 'all', 'criminal', 'minds', 'thoughts', 'destruction', 'mayhem', 'absolute', 'disservice', 'things', 'do', 'day', 'One', 'person', 'even', 'went', 'so', 'far', 'as', 'say', 'members', 'personal', 'gain', 'Wow', '!', 'Entry', 'level', 'personnel', 'just', 'under', '$', '8.00', 'assuming', '40', 'work', 'week', 'Of', 'course', 'many', 'much', 'more', 'those', 'harm', 'way', 'typically', 'put', '16-18', 'end', 'pay', 'well', 'wage', 'So', 'beg', 'please', 'yourself', 'familiar', 'around', 'Go', 'nearby', 'visitor', 'pass', 'meet', 'some', 'quick', 'disparage', 'You', 'surprised', 'no', 'longer', 'accepts', 'lieu', 'prison', 'They', 'require', 'GED', 'prefer', 'high', 'school', 'diploma', 'middle', 'ranks', 'expected', 'undergraduate', 'degrees', 'upper', 'encouraged', 'advanced']\n","frequency <= 2; 234\n","common_words are  ['the', 'for', 'this', 'movie', ',', 'I', 'not', 'I', 'be', ',', 'or', '.', 'of', 'people', 'who', 'a', 'about', 'the', 'military', 'or', 'who', 'they', 'they', 'on', 'this', 'or', 'on', 'about', 'Abu-Gharib', 'makes', 'about', 'the', 'of', 'in', 'the', '.', 'the', 'I', 'this', 'the', 'number', 'of', 'people', 'in', 'the', 'US', 'military', 'million', 'on', 'with', 'in', 'the', 'and', 'for', 'a', 'total', 'of', 'million', '.', 'The', 'number', 'of', 'people', 'for', 'at', 'at', 'Abu-Gharib', 'makes', 'the', 'total', 'of', 'people', 'of', 'the', 'total', 'military', '.', 'you', 'every', 'military', 'that', 'in', 'to', 'Abu-Gharib', ',', 'you', 'would', 'not', 'to', 'that', 'a', 'number', '.', 'The', 'in', 'this', 'movie', 'would', 'to', '.', 'I', 'that', 'it', 'to', 'be', ',', 'in', ',', 'the', 'and', 'are', 'to', 'make', 'about', 'the', 'of', 'the', 'military', 'an', 'to', '.', ',', 'the', 'US', 'military', 'at', 'there', 'are', 'not', 'on', '.', 'The', 'military', 'the', 'for', 'and', '.', 'the', ',', 'the', ',', 'the', 'US', 'military', 'was', 'the', 'on', 'the', '.', 'the', 'of', 'the', 'the', ',', 'it', 'was', 'military', 'who', 'at', 'people', ',', 'the', 'people', 'this', 'movie', ',', 'and', 'make', 'it', '.', ',', 'was', '.', 'days', ',', ',', 'and', 'a', 'was', 'and', '.', 'and', 'days', ',', 'not', 'and', '.', 'there', 'are', 'people', 'in', 'the', 'US', 'military', '.', ',', 'there', 'are', 'in', 'every', 'of', ',', 'every', '.', 'to', 'people', 'on', 'this', 'that', 'million', 'and', 'are', ',', 'with', 'on', 'of', 'or', 'an', 'to', 'the', 'that', 'they', 'every', '.', 'on', 'this', 'to', 'that', 'military', 'are', 'in', 'it', 'for', '.', 'make', 'an', 'hour', 'a', 'hour', '.', ',', 'a', 'and', 'in', 'in', 'hour', 'days', 'for', 'on', '.', 'makes', 'the', 'minimum', '.', 'for', '.', 'I', 'you', ',', 'make', 'with', 'the', 'you', '.', 'to', 'a', ',', 'get', 'a', 'and', 'of', 'the', 'and', 'you', 'are', 'to', '.', 'would', 'be', '.', 'The', 'military', 'people', 'in', 'of', '.', 'a', 'minimum', 'of', 'a', 'and', 'a', '.', 'The', 'are', 'to', 'get', 'a', 'minimum', 'of', 'and', 'the', 'are', 'to', 'get', '.']\n","frequency >= 3;  306\n"]}]},{"cell_type":"code","source":["# chapter 2, lesson 4 정제(cleaning)-lesson 5 실습; cleaning by word lengths\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","#from text import TEXT\n","#nltk.download('punkt')\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_words = word_tokenize(corpus)\n","\n","def clean_by_freq(tokenized_words, cut_off_count):\n","    vocab = Counter(tokenized_words)\n","\n","    uncommon_words = [key for key, value in vocab.items() if value <= cut_off_count]\n","    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n","\n","    return cleaned_words\n","\n","def clean_by_len(tokenized_words, cut_off_length):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        if len(word) > cut_off_length:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","clean_by_freq = clean_by_freq(tokenized_words, 2)\n","cleaned_words = clean_by_len(clean_by_freq, 2)\n","\n","#cleaned_words"],"metadata":{"id":"QxoLBe1uG_7g","executionInfo":{"status":"ok","timestamp":1700815622801,"user_tz":-540,"elapsed":9,"user":{"displayName":"M J","userId":"09515631461713927918"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# chapter 2, lesson 6 불용어(stopwords)\n","from nltk.corpus import stopwords\n","#nltk.download('stopwords')  # 코퍼스의 종류에 상관없이 많이 사용되는 불용어 179개를 제공\n","\n","stopwords_set = set(stopwords.words('english'))\n","\n","print('stopwords count :', len(stopwords_set))\n","print(stopwords_set)\n","\n","stopwords_set.add('hello')\n","stopwords_set.remove('the')\n","stopwords_set.remove('me')\n","\n","#print('stopwords count is', len(stopwords_set))\n","#print('stopwords are', stopwords_set)\n","\n","cleaned_words = []\n","\n","for word in cleaned_by_freq:\n","    if word not in stopwords_set:\n","        cleaned_words.append(word)\n","\n","print('불용어 제거 전; ', len(cleaned_by_freq))\n","print('불용어 제거 후; ', len(cleaned_words))"],"metadata":{"id":"tLWmUukdCogn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700815622801,"user_tz":-540,"elapsed":9,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"bc19ab0f-5b0c-40e0-83c2-58301de4413d"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["stopwords count : 179\n","{'i', 'me', 'in', 'here', 'from', 'on', 'any', 'off', 'at', 'each', 'when', 'very', 'own', 'does', 'because', 'why', 'who', 'your', 'most', 'those', 'her', \"aren't\", 'were', 'where', 'll', 'yours', 'against', 'am', 'some', 'hadn', 'being', 'same', 'above', 'these', 'did', 'was', 'with', 'not', 'once', 'ourselves', 'doesn', 'more', 'wasn', 'myself', \"don't\", 'ours', \"shan't\", 'having', 'while', 'about', 'what', 'which', 's', \"you're\", 'should', 'didn', \"hadn't\", 'hasn', 'wouldn', \"you'll\", 'our', 'out', 'ma', 'y', 'shouldn', \"that'll\", 'mustn', \"shouldn't\", 'have', \"wouldn't\", \"it's\", 'do', 'both', 'won', 'over', 'themselves', 'couldn', 'for', 'been', 'whom', 'theirs', 'himself', 'she', 'itself', 'than', \"weren't\", \"you'd\", 'further', 'isn', \"didn't\", 'haven', 'is', 're', 'during', 'herself', 'all', 'just', 't', 'mightn', 'my', 'or', 'them', 'a', 'will', 'can', 'between', 'below', 'few', 'weren', \"won't\", 've', \"mustn't\", 'no', \"should've\", 'we', 'by', 'yourselves', 'through', 'aren', \"isn't\", 'shan', 'to', 'had', 'into', 'o', \"she's\", 'hers', 'before', 'again', 'so', 'of', 'then', 'd', 'him', 'it', \"hasn't\", 'has', 'its', 'but', 'and', 'he', 'as', 'down', 'now', \"couldn't\", \"doesn't\", 'such', 'an', \"mightn't\", 'after', 'if', 'only', \"needn't\", 'the', 'that', 'up', 'until', 'their', 'under', 'yourself', 'ain', 'this', \"wasn't\", \"haven't\", 'be', 'too', 'm', 'nor', \"you've\", 'you', 'how', 'are', 'his', 'doing', 'there', 'other', 'don', 'they', 'needn'}\n","불용어 제거 전;  306\n","불용어 제거 후;  155\n"]}]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리 07.불용어제거실습\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","#from text import TEXT\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","corpus = TEXT\n","tokenized_words = word_tokenize(TEXT)\n","\n","# NLTK에서 제공하는 불용어 목록을 세트 자료형으로 받아와 주세요\n","stopwords_set = set(stopwords.words('english'))\n","\n","def clean_by_stopwords(tokenized_words, stopwords_set):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        # 여기에 코드를 작성하세요\n","        if word not in stopwords_set:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","# 테스트 코드\n","#clean_by_stopwords(tokenized_words, stopwords_set)"],"metadata":{"id":"9jeoAkLAHKFK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700815622801,"user_tz":-540,"elapsed":6,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"ad206d77-6caf-4b34-cf7b-b923881e9810"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리  레슨 08.정규화\n","text = \"What can I do for you? Do your homework now.\"\n","print(text.lower()) # 대소문자 통합\n","\n","synonym_dict = {'US':'USA', 'U.S':'USA', 'Ummm':'Umm', 'Ummmm':'Umm'}\n","text = \"She became a US citizen. Ummmm, I think, maybe and or.\"\n","normalized_words = []\n","\n","tokenized_words = nltk.word_tokenize(text)\n","\n","for word in tokenized_words:\n","    if word in synonym_dict.keys():\n","        word = synonym_dict[word]\n","\n","    normalized_words.append(word)\n","print(normalized_words)"],"metadata":{"id":"L4dJ6vRKzMIU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700815622801,"user_tz":-540,"elapsed":5,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"a376ea77-f14b-408b-cc96-079868b473e8"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["what can i do for you? do your homework now.\n","['She', 'became', 'a', 'USA', 'citizen', '.', 'Umm', ',', 'I', 'think', ',', 'maybe', 'and', 'or', '.']\n"]}]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리 레슨 09.어간추출 (stemming)\n","from nltk.tokenize import word_tokenize\n","import nltk\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","from nltk.stem import PorterStemmer  # 단순히 어미만 잘라내는 방식\n","from nltk.stem import LancasterStemmer\n","\n","porter_stemmer = PorterStemmer()\n","lancaster_stemmer = LancasterStemmer()\n","text = \"You are so lovely. I am loving you now.\"\n","porter_stemmed_words = []\n","lancaster_stemmed_words = []\n","\n","tokenized_words = nltk.word_tokenize(text)\n","\n","for word in tokenized_words:\n","#    stem = porter_stemmer.stem(word)\n","#    porter_stemmed_words.append(stem)\n","    porter_stemmed_words.append(porter_stemmer.stem(word))\n","    lancaster_stemmed_words.append(lancaster_stemmer.stem(word))\n","\n","#for word in tokenized_words:\n","#    stem = lancaster_stemmer.stem(word)\n","#    lancaster_stemmed_words.append(stem)\n","\n","print('before; ', tokenized_words)\n","print('porter; ', porter_stemmed_words)\n","print('lancaster; ', lancaster_stemmed_words)\n","\n"],"metadata":{"id":"Ukn6cI7oLHWH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701048189351,"user_tz":-540,"elapsed":440,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"6e754cb8-1783-40cf-e12e-e939ce702f67"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["before;  ['You', 'are', 'so', 'lovely', '.', 'I', 'am', 'loving', 'you', 'now', '.']\n","porter;  ['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']\n","lancaster;  ['you', 'ar', 'so', 'lov', '.', 'i', 'am', 'lov', 'you', 'now', '.']\n"]}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","# 포터 스테머 어간 추출 함수\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        # porter_stemmed_words.append(porter_stemmer.stem(word))\n","        stem = porter_stemmer.stem(word)\n","        porter_stemmed_words.append(stem)\n","\n","    return porter_stemmed_words"],"metadata":{"id":"w2Yc9dCyQ6g8","executionInfo":{"status":"ok","timestamp":1700815622801,"user_tz":-540,"elapsed":3,"user":{"displayName":"M J","userId":"09515631461713927918"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# chapter 2, lesson 10 어간 추출 실습\n","# 필요한 패키지와 함수 불러오기\n","import nltk\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","#from text import TEXT\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_words = word_tokenize(corpus)\n","\n","# 포터 스테머의 어간 추출\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        porter_stemmed_words.append(porter_stemmer.stem(word))\n","\n","    return porter_stemmed_words\n","\n","stemming_by_porter(tokenized_words)"],"metadata":{"id":"e4HlCT3wR_6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# chapter 2, lesson 11 자연어 전처리 적용1\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# .py 모듈 수정 시 자동 리로드\n","#%load_ext autoreload\n","#%autoreload 2\n","\n","#from preprocess import clean_by_freq\n","#from preprocess import clean_by_len\n","#from preprocess import clean_by_stopwords\n","\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","# 1. normalization\n","df['review'] = df['review'].str.lower()\n","\n","# 2. tokenize\n","df['word_tokens'] = df['review'].apply(word_tokenize)\n","\n","# 3. cleaning\n","stopwords_set = set(stopwords.words('english'))\n","df['cleaned_tokens'] = df['word_tokens'].apply(lambda x: clean_by_freq(x, 1))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n","\n","# 4. stemming\n","df['stemmed_tokens'] = df['cleaned_tokens'].apply(stemming_by_porter)\n","\n","df['stemmed_tokens'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2zlhyrTUZT8","executionInfo":{"status":"ok","timestamp":1701049508190,"user_tz":-540,"elapsed":619,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"a48ad805-a913-4ab5-e06b-6b791c13a00e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","<ipython-input-8-3b7ed36f0ca4>:17: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"execute_result","data":{"text/plain":["['one',\n"," 'film',\n"," 'said',\n"," 'realli',\n"," 'bad',\n"," 'movi',\n"," 'like',\n"," 'said',\n"," 'realli',\n"," 'bad',\n"," 'movi',\n"," 'bad',\n"," 'one',\n"," 'film',\n"," 'like']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# .py 모듈 수정 시 자동 리로드\n","%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"iMQXll-xWl23","executionInfo":{"status":"ok","timestamp":1701049649907,"user_tz":-540,"elapsed":317,"user":{"displayName":"M J","userId":"09515631461713927918"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## chapter 3. 문장 단위 전처리"],"metadata":{"id":"YhJ_wUjVfrX1"}},{"cell_type":"code","source":["# chapter 3, lesson 2 문장 토큰화 실습\n","# sentence tokenization\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_sents = sent_tokenize(corpus)\n","\n","tokenized_sents"],"metadata":{"id":"CuRZcVRPiATn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# chapter 3, lesson 3 품사태깅(POS tagging)\n","from nltk.tag import pos_tag  # part of speech tagging 품사태깅. 각 품사태그는 Penn Treebank POS Tags 기준\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","import nltk\n","#nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","\n","TEXT = \"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\"Hey, let\\'s pool our money together and make a really bad movie!\\\" Or something like that.\"\n","\n","corpus = TEXT\n","pos_tagged_words = []\n","\n","tokenized_sents = sent_tokenize(corpus)\n","for sentence in tokenized_sents:\n","    # word tokenize\n","    tokenized_words = word_tokenize(sentence)\n","#    print(tokenized_words)\n","\n","    # pos\n","    pos_tagged = pos_tag(tokenized_words)\n","    pos_tagged_words.extend(pos_tagged)\n","\n","print(pos_tagged_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TQZRnJMijp0K","executionInfo":{"status":"ok","timestamp":1701052203034,"user_tz":-540,"elapsed":331,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"14c1de7e-6e9e-4bfb-b44f-c50fb792ee3c"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[('Watching', 'VBG'), ('Time', 'NNP'), ('Chasers', 'NNPS'), (',', ','), ('it', 'PRP'), ('obvious', 'VBZ'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('made', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('friends', 'NNS'), ('.', '.'), ('Maybe', 'RB'), ('they', 'PRP'), ('were', 'VBD'), ('sitting', 'VBG'), ('around', 'IN'), ('one', 'CD'), ('day', 'NN'), ('in', 'IN'), ('film', 'NN'), ('school', 'NN'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Hey', 'NNP'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('pool', 'VB'), ('our', 'PRP$'), ('money', 'NN'), ('together', 'RB'), ('and', 'CC'), ('make', 'VB'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('!', '.'), (\"''\", \"''\"), ('Or', 'CC'), ('something', 'NN'), ('like', 'IN'), ('that', 'DT'), ('.', '.')]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"code","source":["# chapter3, lesson5 표제어 추출 (Lemmatization); 표제어(lemma)란 사전적 어원. am, are, is -> be. 단어의 정규화\n","\n","from nltk.tokenize import word_tokenize\n","import nltk\n","#nltk.download('punkt')\n","from nltk.tag import pos_tag  # Penn Treebank POS(part of speech) Tag\n","from nltk.corpus import wordnet as wn  # WordNet POS Tag\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","text = 'You are the happiest person.'\n","tokenize_words = word_tokenize(text)\n","\n","tagged_words = pos_tag(tokenize_words)\n","print(tagged_words)\n","\n","def penn_to_wn(tag):\n","    if tag.startswith('J'):\n","        return wn.ADJ\n","    elif tag.startswith('N'):\n","        return wn.NOUN\n","    elif tag.startswith('R'):\n","        return wn.ADV\n","    elif tag.startswith('V'):\n","        return wn.VERB\n","    else:\n","        return\n","\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_words = []\n","\n","for word, tag in tagged_words:\n","    wn_tag = penn_to_wn(tag)\n","    if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","        lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n","    else:\n","        lemmatized_words.append(word)\n","\n","print('before; ', tokenize_words)\n","print('after; ', lemmatized_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvn1Y_TKfjyE","executionInfo":{"status":"ok","timestamp":1701052561283,"user_tz":-540,"elapsed":1502,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"78ef4890-0eb8-4343-ede8-de203fe35726"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["[('You', 'PRP'), ('are', 'VBP'), ('the', 'DT'), ('happiest', 'JJS'), ('person', 'NN'), ('.', '.')]\n","before;  ['You', 'are', 'the', 'happiest', 'person', '.']\n","after;  ['You', 'be', 'the', 'happy', 'person', '.']\n"]}]},{"cell_type":"code","source":["# chapter 3, lesson 7 자연어 전처리 적용 II ~ lesson 8 자연어 전처리 후 통합하기\n","# nlp preprocessing II\n","\n","import pandas as pd\n","#import nltk\n","#from nltk.tokenize import word_tokenize\n","#from nltk.tokenize import sent_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","#from nltk.corpus import stopwords\n","#nltk.download('stopwords')\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n","\n","# sentence tokenization\n","df['review'] = df['review'].str.lower()\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)\n","#print(df['sent_tokens'][0])\n","\n","# 품사 태깅 pos_tagging\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n","#print(df['pos_tagged_tokens'][0])\n","\n","# 표제어 추출 Lemmatization\n","df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(word_lemmatizer)\n","#print(df['lemmatized_tokens'][0])\n","\n","#\n","stopwords_set = set(stopwords.words('english'))\n","df['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x: clean_by_freq(x, 1))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n","df[['cleaned_tokens']]\n","\n","# combination\n","#df['combined_corpus'] = df['cleaned_tokens'].apply(combine)\n","#df[['combined_corpus']]"],"metadata":{"id":"wq59gynTnwFB","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1701075888363,"user_tz":-540,"elapsed":785,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"399c9a9b-3de4-43d9-fdd3-10f573214004"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-46-8e98049efed2>:12: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n"]},{"output_type":"execute_result","data":{"text/plain":["                                      cleaned_tokens\n","0  [make, one, film, say, make, really, bad, movi...\n","1                                       [film, film]\n","2  [new, york, joan, barnard, elvire, audrey, bar...\n","3  [film, film, jump, send, n't, jump, radio, n't...\n","4  [site, movie, bad, even, movie, movie, make, m...\n","5  [ehle, northam, wonderful, wonderful, ehle, no...\n","6  [role, movie, n't, author, book, funny, author...\n","7  [plane, ceo, search, rescue, mission, call, ce...\n","8  [gritty, movie, movie, keep, sci-fi, good, kee...\n","9                                       [girl, girl]"],"text/html":["\n","  <div id=\"df-cd94d01b-d7b4-4e28-a812-54f3da54c875\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cleaned_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[make, one, film, say, make, really, bad, movi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[film, film]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[film, film, jump, send, n't, jump, radio, n't...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[site, movie, bad, even, movie, movie, make, m...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>[ehle, northam, wonderful, wonderful, ehle, no...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>[role, movie, n't, author, book, funny, author...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>[plane, ceo, search, rescue, mission, call, ce...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>[gritty, movie, movie, keep, sci-fi, good, kee...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>[girl, girl]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd94d01b-d7b4-4e28-a812-54f3da54c875')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cd94d01b-d7b4-4e28-a812-54f3da54c875 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cd94d01b-d7b4-4e28-a812-54f3da54c875');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-6c6bb6f4-bf11-4cce-b95a-e45f20bf791d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6c6bb6f4-bf11-4cce-b95a-e45f20bf791d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-6c6bb6f4-bf11-4cce-b95a-e45f20bf791d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["# Test APPLY function\n","# apply를 하면, 데이터 구조를 따로 고려하지 않고도, 리스트의 엔티티별로 해당 함수를 적용해줌\n","\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n","tokens = []\n","\n","df['review'] = df['review'].str.lower()\n","df['review'][0]\n","for i in range(0, 10):\n","    # print(sent_tokenize(df['review'][i]))\n","    tokens.append(sent_tokenize(df['review'][i]))\n","\n","print(tokens[4])\n","#df['sent_tokens'] = sent_tokenize(df['review'][0])\n","#df['sent_tokens']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VHPAYz3w-lU0","executionInfo":{"status":"ok","timestamp":1701075864196,"user_tz":-540,"elapsed":638,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"7251b499-1953-4a2c-e252-b2abaed4155f"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["['\"yes, i agree with everyone on this site this movie is very very bad.', 'to even call this a movie is an insult to all movies ever made.', \"it's 40 minutes long.\", 'someone compares this movie to an after school special.', 'b-i-n-g-o!', 'that describes is perfectly.', 'the packaging for this movie intentionally is misleading.', 'for example, the title of this movie should describe the movie.', 'rubberface???', 'that should be the first hint.', 'it was retitled with a new package of some goofy face jim probably made in his stand-up days.', 'i was hoping for more stand-up from jim.', 'if you like jim now as an actor.', 'you would love him in his stand up days.', 'still trying to locate the rodney dangerfield young comedians special from hbo that featured jim in his early career days.', \"it isn't even mentioned on this site.\", \"i'd love to find anything jim did stand-up wise.\", 'also jim carrey is a supporting actor in this movie.', 'the main character is very very annoying.', 'she is some girl lacking self confidence but yet wants to be a stand up comedian.', 'jim is there to say lines like \\\\\"\"that\\'s funny janet\\\\\"\" and \\\\\"\"you really are talented\\\\\"\".', 'and honestly she is terrible really terrible.', 'and the movie is terrible.', 'beware of false advertising and a really bad movie.\"']\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-44-801aa8c2ce09>:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n"]}]},{"cell_type":"markdown","source":["## chapter 4. 자연어 숫자로 표현하기"],"metadata":{"id":"kmbBPYiKDRyp"}},{"cell_type":"code","source":["# chapter 4, lesson 1 정수 인코딩 Integer Encoding; 텍스트를 숫자데이터로 변환하는 방법. 토큰화된 각 단어에 특정 정수를 매핑\n","\n","#tokens = df['cleaned_tokens'][4]\n","tokens = sum(df['cleaned_tokens'], [])  # whole token summed list\n","#print('tokens are ', tokens)\n","\n","vocab = Counter(tokens)\n","#print('vocab is ', vocab)\n","vocab = vocab.most_common()\n","#print('most_common vocab is ', vocab)\n","\n","word_to_idx = {}\n","i = 0\n","\n","for (word, frequency) in vocab:\n","    i += 1  # 0은 아무 의미 없는 (무시되는) 정수를 위해 남겨두고, 1부터 시작\n","    word_to_idx[word] = i\n","\n","print(word_to_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHSv1Qm2Xzsh","executionInfo":{"status":"ok","timestamp":1701076617601,"user_tz":-540,"elapsed":511,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"fccf0187-09b9-4701-f89b-544b828b27bf"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["{'movie': 1, 'film': 2, \"n't\": 3, 'scene': 4, 'bad': 5, 'time': 6, 'reason': 7, 'make': 8, 'jim': 9, 'good': 10, 'one': 11, 'like': 12, 'could': 13, \"'re\": 14, 'quastel': 15, 'really': 16, 'even': 17, 'monster': 18, 'joan': 19, 'love': 20, 'author': 21, 'try': 22, 'dialogue': 23, 'idea': 24, 'italy': 25, 'colleague': 26, 'maggot': 27, 'end': 28, 'watch': 29, 'jump': 30, 'radio': 31, 'stand-up': 32, 'day': 33, 'terrible': 34, 'ehle': 35, 'northam': 36, 'search': 37, 'rescue': 38, 'call': 39, 'knowles': 40, 'henriksen': 41, 'easily': 42, 'see': 43, 'appear': 44, 'get': 45, 'character': 46, 'think': 47, 'use': 48, 'whether': 49, 'need': 50, 'though': 51, 'sci-fi': 52, 'look': 53, 'say': 54, 'new': 55, 'york': 56, 'barnard': 57, 'elvire': 58, 'audrey': 59, 'john': 60, 'saxon': 61, 'etruscan': 62, 'tomb': 63, 'drug': 64, 'story': 65, 'romantic': 66, 'waste': 67, 'etrusco': 68, 'send': 69, 'reporter': 70, 'fear': 71, 'site': 72, 'special': 73, 'describe': 74, 'actor': 75, 'stand': 76, 'comedian': 77, 'wonderful': 78, 'lust': 79, 'role': 80, 'book': 81, 'funny': 82, 'queen': 83, 'corn': 84, 'plane': 85, 'ceo': 86, 'mission': 87, 'harlan': 88, 'lance': 89, 'put': 90, 'wood': 91, 'two': 92, 'decent': 93, 'sasquatch': 94, 'edit': 95, 'want': 96, 'potential': 97, 'material': 98, 'relate': 99, 'crib': 100, 'exposition': 101, 'far': 102, 'costume': 103, 'would': 104, 'stereotype': 105, 'well': 106, 'effective': 107, 'occur': 108, 'line': 109, 'back': 110, 'irrelevant': 111, 'comment': 112, 'cut': 113, 'random': 114, 'show': 115, 'important': 116, 'either': 117, 'never': 118, 'leave': 119, 'gritty': 120, 'keep': 121, 'suspense': 122, 'girl': 123}\n"]}]},{"cell_type":"code","source":["def idx_encoder(tokens, word_to_idx):\n","    encoded_idx = []\n","    for token in tokens:\n","        idx = word_to_idx[token]\n","        encoded_idx.append(idx)\n","    return encoded_idx\n","\n","df['integer_encoded'] = df['cleaned_tokens'].apply(lambda x: idx_encoder(x, word_to_idx))\n","print(df[['integer_encoded']])\n"],"metadata":{"id":"OAjhKYOCZX9X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# chapter 4, lesson 2 정수 인코딩 실습\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","word_to_idx = {} # 단어 별 인덱스 부여하기 위한 딕셔너리\n","i = 0\n","encoded_idx = [] # 각 토큰의 정수 인덱스를 부여하기 위한 리스트\n","corpus = TEXT\n","\n","tokenized_words = word_tokenize(corpus)\n","\n","# 단어의 빈도수를 계산하여 정렬하는 코드를 작성하세요\n","vocab = Counter(tokenized_words)\n","vocab = vocab.most_common()\n","\n","for (word, frequency) in vocab:\n","    # 여기에 코드를 작성하세요\n","    i += 1\n","    word_to_idx[word] = i\n","\n","for word in tokenized_words:\n","    # 여기에 코드를 작성하세요\n","    idx = word_to_idx[word]\n","    encoded_idx.append(idx)\n","\n","# 테스트 코드\n","encoded_idx"],"metadata":{"id":"xMuFgsuHbwuO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## chapter 5. 감성 분석 sentiment analysis\n","감성 분석을 위해선 코퍼스에 포함되는 특정 단어의 감성을 판단하는 기준이 필요\n","\n","### 기준을 만드는 접근법 2가지\n","* 규칙 기반 감성 분석; 감성 어휘 사전(사람이 특정 단어를 보고 직접 긍정/부정/중립 수치를 기재해 놓은 단어들의 집합)을 기준으로 단어의 긍부정을 분류\n","* 머신러닝 기반 감성 분석; 다수의 코퍼스들을 통해 긍정단어와 부정단어를 구분하는 모델을 (당연히 정답 데이터로) 학습시켜 그 모델을 기반으로 감성지수를 확인\n","\n","### 어휘사전 (nltk.corpus)\n","대표적인 감성 어휘 사전으로 SentiWordNet이 있음. WordNet/SentiWordNet은 NLTK에서 제공하는 대규모 영어 어휘 사전. 단어의 품사에 따라 감성 지수는 달라지므로, 품사의 synset을 정확히 지정하는 것이 중요\n","* WordNet/Synset(Sets of Cognitive Synonyms)\n","    * 단어, 품사, 순번; 일반적으로 더 많이 사용되는 의미가 앞 순번으로 부여. 의미가 비슷한 다른 단어들도 목록에 포함\n","* SentiWordNet/SentiSynset(WordNet과 유사하나, 0~1사이의 긍정지수/부정지수/객관성지수를 할당. 긍정$-$부정으로 판단)\n","    * 긍정지수 pos_score, 부정지수 neg_score, 객관성지수 obj_score\n","\n","### **VADER(Valence Aware Dictionary and sEntiment Reasoner)**\n","* 감성 분석을 위한 어휘 사전이자 알고리즘\n","* SentiWordNet과의 큰 차이점은 일반적인 감성 어휘 사전의 규칙 외에도 축약형과 기호 등을 고려해 감성 지수를 추출할 수 있다는 점\n","* 그래서 주로 축약형 표현이나 특수 문자가 많이 사용된 소셜 미디어 텍스트를 분석할 때 자주 사용됨\n","* SentiWordNet은 단어의 감성지수만 확인할 수 있기때문에, 코퍼스의 감성 지수도 각 단어의 감성 지수 합으로 계산했었으나, VADER는 단어, 문장, 여러문장으로 이루어진 코퍼스로 바로 감성 지수를 계산할 수 있음\n","* 심지어 코퍼스를 단어 단위로 토큰화해 파라미터로 전달할 필요 없이, VADER 내부 동작에서 필요한 토큰화와 감성 지수 추출 작업을 알아서 해 줌\n","* 결과를 딕셔너리 형태의 점수들로 반환 (neg;부정, neu;중립, pos;긍정, compound; 세 지수 조합 (-1 부정~1 긍정)"],"metadata":{"id":"ZGdJ9jPJkKe8"}},{"cell_type":"code","source":["# chapter 5, lesson 3 SentiWordNet\n","# WordNet의 Synset과 SentiWordNet의 SentiSynset의 결과 비교\n","import nltk\n","from nltk.corpus import wordnet as wn\n","from nltk.corpus import sentiwordnet as swn\n","from nltk import pos_tag\n","\n","nltk.download('wordnet')\n","nltk.download('sentiwordnet')\n","nltk.download('omw-1.4')\n","\n","#word = 'happy'\n","word = 'hard'\n","#print(\"wordnet-{}: \".format(word), wn.synsets(word))\n","#print(\"sentiwordnet-{}: \".format(word), list(swn.senti_synsets(word)))\n","\n","# happy의 긍정, 부정, 중립 지수 확인하기\n","word_sentisynsets = list(swn.senti_synsets(word))\n","\n","pos_score = word_sentisynsets[0].pos_score()\n","neg_score = word_sentisynsets[0].neg_score()\n","obj_score = word_sentisynsets[0].obj_score()\n","\n","print(pos_score, neg_score, obj_score)\n","sentiment_score = pos_score - neg_score\n","print('sentiment_score = pos_score - neg_score: ', sentiment_score)\n","\n","\n","# 품사 별 감성 지수 비교\n","adj_synsets = wn.synsets(word, wn.ADJ)\n","print('adj_synsets of {} is ...\\n'.format(word), adj_synsets)\n","adv_synsets = wn.synsets(word, wn.ADV)\n","print('adv_synsets of {} is ...\\n'.format(word), adv_synsets)\n","\n","adj_synset = adj_synsets[0]\n","adv_synset = adv_synsets[0]\n","\n","adj_senti_synset = swn.senti_synset(adj_synset.name())\n","adv_senti_synset = swn.senti_synset(adv_synset.name())\n","print(adj_senti_synset, adv_senti_synset)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_LzlVV0sN4h","executionInfo":{"status":"ok","timestamp":1701218721818,"user_tz":-540,"elapsed":5744,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"994abcb4-460b-463f-ab79-df9736de81fc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["0.0 0.75 0.25\n","sentiment_score = pos_score - neg_score:  -0.75\n","adj_synsets of hard is ...\n"," [Synset('difficult.a.01'), Synset('hard.a.02'), Synset('hard.a.03'), Synset('hard.s.04'), Synset('arduous.s.01'), Synset('unvoiced.a.01'), Synset('hard.a.07'), Synset('hard.a.08'), Synset('intemperate.s.03'), Synset('hard.s.10'), Synset('hard.s.11'), Synset('hard.s.12')]\n","adv_synsets of hard is ...\n"," [Synset('hard.r.01'), Synset('hard.r.02'), Synset('hard.r.03'), Synset('hard.r.04'), Synset('hard.r.05'), Synset('heavily.r.07'), Synset('hard.r.07'), Synset('hard.r.08'), Synset('hard.r.09'), Synset('hard.r.10')]\n","<difficult.a.01: PosScore=0.0 NegScore=0.75> <hard.r.01: PosScore=0.125 NegScore=0.125>\n"]}]},{"cell_type":"code","source":["# chapter 5,\n","from nltk.corpus import sentiwordnet as swn\n","word = 'love'\n","pos = wn.VERB\n","\n","word_synsets = wn.synsets(word, pos)\n","\n","word_synset = word_synsets[0]\n","word_senti_synset = swn.senti_synset(word_synset.name())\n","\n","pos_score = word_senti_synset.pos_score()\n","neg_score = word_senti_synset.neg_score()\n","\n","sentiment_score = pos_score - neg_score\n","print(sentiment_score)\n","\n","# 또는 --------\n","word_sentisynsets = list(swn.senti_synsets(word, pos))\n","pos_score = word_sentisynsets[0].pos_score()\n","neg_score = word_sentisynsets[0].neg_score()\n","print(pos_score-neg_score)\n","# --------\n","\n","#word_synsets =  wn.synsets(word, pos)\n","#word_synset = word_synsets[0]\n","#print(word_synset)\n","#word_senti_synset = swn.senti_synset(word_synsets[0].name())\n","#word_senti_synset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_1h4tUW108I","executionInfo":{"status":"ok","timestamp":1701150176441,"user_tz":-540,"elapsed":544,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"9a1afdd9-bbc4-4f26-d47d-11c661f35785"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["0.5\n","0.5\n"]}]},{"cell_type":"code","source":["# chapter 5, lesson 5 감성 분석 적용\n","import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","#print('df is \\n', df)\n","\n","# sentence tokenization; sentence 별로 분리하는 작업\n","df['review'] = df['review'].str.lower()\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)\n","#print(\"df['sent_tokens'] is \\n\", df['sent_tokens'])\n","\n","# 품사 태깅 pos_tagging\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n","#print(\"df['pos_tagged_tokens'] is \\n\", df['pos_tagged_tokens'])\n","\n","# 표제어 추출 Lemmatization\n","df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(word_lemmatizer)\n","#print(df['lemmatized_tokens'][0])\n","\n","#\n","stopwords_set = set(stopwords.words('english'))\n","df['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x: clean_by_freq(x, 1))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n","\n","pos_tagged_words = df['pos_tagged_tokens'][0]\n","senti_score = 0\n","\n","for word, tag in pos_tagged_words:\n","    wn_tag = penn_to_wn(tag)  # penn tree bank 기준 품사를 wordnet 기준 품사로 변경\n","    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","        continue\n","\n","    if not wn.synsets(word, wn_tag):\n","        continue\n","    else:\n","        synsets = wn.synsets(word, wn_tag)\n","\n","    synset = synsets[0]\n","    #    print(synset.name())\n","    swn_synset = swn.senti_synset(synset.name())\n","\n","    word_senti_score = (swn_synset.pos_score() - swn_synset.neg_score())\n","    senti_score += word_senti_score\n","\n","print(senti_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVlhu9ydnKBH","executionInfo":{"status":"ok","timestamp":1701656390861,"user_tz":-540,"elapsed":358,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"fba7ac3f-b00b-45a6-8813-bbdd0414b3c7"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["-0.375\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-81c7b2de43c5>:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]}]},{"cell_type":"code","source":["# chapter 5, lesson 6 감성 분석 결과 확인\n","#from preprocess import swn_polarity\n","import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","#print('df is \\n', df)\n","index = 8\n","\n","# sentence tokenization; sentence 별로 분리하는 작업\n","df['review'] = df['review'].str.lower()\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)\n","#print(\"df['sent_tokens'] is \\n\", df['sent_tokens'][index])\n","\n","# 문장 안에서의 품사 태깅 pos_tagging (pos ; part of speech)\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n","#print(\"df['pos_tagged_tokens'][{}] is {}\\n\".format(index, df['pos_tagged_tokens'][index]))\n","#print(\"df['pos_tagged_tokens'] is \\n\", df['pos_tagged_tokens'])\n","\n","df['swn_sentiment'] = df['pos_tagged_tokens'].apply(swn_polarity)\n","print('index; {}. sentiment; {}'.format(index, df.iloc[index][['review', 'swn_sentiment']]))\n","\n","#df['review'][index]\n","print(\"df['review'] is \", df['review'])\n","#swn_polarity['review'][1]"],"metadata":{"id":"09hZLhtxvNXG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701658557567,"user_tz":-540,"elapsed":462,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"fd4997ea-7e7d-44f6-9ca5-c74f26f39210"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["index; 8. sentiment; review           a well made, gritty science fiction movie, it ...\n","swn_sentiment                                                  4.5\n","Name: 8, dtype: object\n","df['review'] is  0    \"watching time chasers, it obvious that it was...\n","1    i saw this film about 20 years ago and remembe...\n","2    minor spoilers in new york, joan barnard (elvi...\n","3    i went to see this film with a great deal of e...\n","4    \"yes, i agree with everyone on this site this ...\n","5    \"jennifer ehle was sparkling in \\\"\"pride and p...\n","6    amy poehler is a terrific comedian on saturday...\n","7    \"a plane carrying employees of a large biotech...\n","8    a well made, gritty science fiction movie, it ...\n","9    \"incredibly dumb and utterly predictable story...\n","Name: review, dtype: object\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-37-7ae2587d26df>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","# import pos_tagger, penn_to_wn from preprocess\n","from nltk.corpus import wordnet as wn\n","from nltk.corpus import sentiwordnet as swn\n","# download nltk.download('punkt', 'wordnet', 'sentiwordnet', averaged_perception_tagger')\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)  # tokenize sentense\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)  # tag 품사 to part of speech\n","#df['pos_tagged_tokens']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"O2c0svKrxVWd","executionInfo":{"status":"ok","timestamp":1680761821143,"user_tz":-540,"elapsed":285,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"1eb9f6e1-5e03-4d7f-d6bb-673aeb7b4f29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-9641686e04bf>:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"execute_result","data":{"text/plain":["                                              review\n","0  \"Watching Time Chasers, it obvious that it was...\n","1  I saw this film about 20 years ago and remembe...\n","2  Minor Spoilers In New York, Joan Barnard (Elvi...\n","3  I went to see this film with a great deal of e...\n","4  \"Yes, I agree with everyone on this site this ...\n","5  \"Jennifer Ehle was sparkling in \\\"\"Pride and P...\n","6  Amy Poehler is a terrific comedian on Saturday...\n","7  \"A plane carrying employees of a large biotech...\n","8  A well made, gritty science fiction movie, it ...\n","9  \"Incredibly dumb and utterly predictable story..."],"text/html":["\n","  <div id=\"df-c6230a62-1a50-4f68-b680-99148aa8a218\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"Watching Time Chasers, it obvious that it was...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I saw this film about 20 years ago and remembe...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Minor Spoilers In New York, Joan Barnard (Elvi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I went to see this film with a great deal of e...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"Yes, I agree with everyone on this site this ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>\"Jennifer Ehle was sparkling in \\\"\"Pride and P...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Amy Poehler is a terrific comedian on Saturday...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>\"A plane carrying employees of a large biotech...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>A well made, gritty science fiction movie, it ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>\"Incredibly dumb and utterly predictable story...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6230a62-1a50-4f68-b680-99148aa8a218')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c6230a62-1a50-4f68-b680-99148aa8a218 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c6230a62-1a50-4f68-b680-99148aa8a218');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# chapter 5, lesson 8 VADER\n","!pip install vaderSentiment"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2SUU9rUVpiQE","executionInfo":{"status":"ok","timestamp":1680752586358,"user_tz":-540,"elapsed":4202,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"3e8c1258-9699-43f1-d75a-6ae144594eb0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from vaderSentiment) (2.27.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (1.26.15)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"]}]},{"cell_type":"code","source":["# chapter 5, lesson 8 VADER\n","import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","\n","senti_analyzer = SentimentIntensityAnalyzer()\n","\n","text1 = \"This is a great movie!\"\n","text2 = \"This is a terrible movie!\"\n","text3 = \"This movie was just okay.\"\n","\n","senti_scores_text1 = senti_analyzer.polarity_scores(text1)\n","senti_scores_text2 = senti_analyzer.polarity_scores(text2)\n","senti_scores_text3 = senti_analyzer.polarity_scores(text3)\n","\n","print(senti_scores_text1)\n","print(senti_scores_text2)\n","print(senti_scores_text3)\n","\n","def vader_sentiment(text):\n","    analyzer = SentimentIntensityAnalyzer()\n","\n","    senti_score = analyzer.polarity_scores(text)['compound']\n","\n","    return senti_score\n","\n","df['vader_sentiment'] = df['review'].apply(vader_sentiment)\n","df[['review', 'swn_sentiment', 'vader_sentiment']]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"id":"uDChKs4iswDb","executionInfo":{"status":"ok","timestamp":1701661581471,"user_tz":-540,"elapsed":509,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"951b45b7-9cf1-4f94-bf75-41e41543ca77"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["{'neg': 0.0, 'neu': 0.406, 'pos': 0.594, 'compound': 0.6588}\n","{'neg': 0.531, 'neu': 0.469, 'pos': 0.0, 'compound': -0.5255}\n","{'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.2263}\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["                                              review  swn_sentiment  \\\n","0  \"watching time chasers, it obvious that it was...         -0.375   \n","1  i saw this film about 20 years ago and remembe...         -1.500   \n","2  minor spoilers in new york, joan barnard (elvi...         -2.250   \n","3  i went to see this film with a great deal of e...         -0.500   \n","4  \"yes, i agree with everyone on this site this ...          3.000   \n","5  \"jennifer ehle was sparkling in \\\"\"pride and p...          6.750   \n","6  amy poehler is a terrific comedian on saturday...          0.750   \n","7  \"a plane carrying employees of a large biotech...          8.750   \n","8  a well made, gritty science fiction movie, it ...          4.500   \n","9  \"incredibly dumb and utterly predictable story...         -1.125   \n","\n","   vader_sentiment  \n","0          -0.9095  \n","1          -0.9694  \n","2          -0.2794  \n","3          -0.9707  \n","4           0.8049  \n","5           0.9494  \n","6           0.8473  \n","7           0.9885  \n","8           0.9887  \n","9          -0.7375  "],"text/html":["\n","  <div id=\"df-0467b416-5f2d-45d2-a7a0-ad13bb4fb5df\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>swn_sentiment</th>\n","      <th>vader_sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"watching time chasers, it obvious that it was...</td>\n","      <td>-0.375</td>\n","      <td>-0.9095</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i saw this film about 20 years ago and remembe...</td>\n","      <td>-1.500</td>\n","      <td>-0.9694</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>minor spoilers in new york, joan barnard (elvi...</td>\n","      <td>-2.250</td>\n","      <td>-0.2794</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i went to see this film with a great deal of e...</td>\n","      <td>-0.500</td>\n","      <td>-0.9707</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"yes, i agree with everyone on this site this ...</td>\n","      <td>3.000</td>\n","      <td>0.8049</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>\"jennifer ehle was sparkling in \\\"\"pride and p...</td>\n","      <td>6.750</td>\n","      <td>0.9494</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>amy poehler is a terrific comedian on saturday...</td>\n","      <td>0.750</td>\n","      <td>0.8473</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>\"a plane carrying employees of a large biotech...</td>\n","      <td>8.750</td>\n","      <td>0.9885</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>a well made, gritty science fiction movie, it ...</td>\n","      <td>4.500</td>\n","      <td>0.9887</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>\"incredibly dumb and utterly predictable story...</td>\n","      <td>-1.125</td>\n","      <td>-0.7375</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0467b416-5f2d-45d2-a7a0-ad13bb4fb5df')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0467b416-5f2d-45d2-a7a0-ad13bb4fb5df button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0467b416-5f2d-45d2-a7a0-ad13bb4fb5df');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d54d09ad-d94c-4058-8c69-73c32cfa74e1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d54d09ad-d94c-4058-8c69-73c32cfa74e1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d54d09ad-d94c-4058-8c69-73c32cfa74e1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["# chapter 5, lesson 9 VADER 감정 분석 실습\n","import nltk\n","import pandas as pd\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","analyzer = SentimentIntensityAnalyzer()\n","\n","def vader_sentiment(text):\n","    # 여기에 코드를 작성하세요\n","    senti_score = analyzer.polarity_scores(text)['compound']\n","    return senti_score\n","\n","df['senti_score'] = df['review'].apply(vader_sentiment)\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454},"id":"BKeRQeJPMrYI","executionInfo":{"status":"ok","timestamp":1701661693599,"user_tz":-540,"elapsed":289,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"d47f0cdf-fb6f-43ca-e0cf-0ae2d505544f"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n","<ipython-input-40-ff5a917666ef>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"execute_result","data":{"text/plain":["                                              review  senti_score\n","0  \"Watching Time Chasers, it obvious that it was...      -0.9095\n","1  I saw this film about 20 years ago and remembe...      -0.9694\n","2  Minor Spoilers In New York, Joan Barnard (Elvi...      -0.2794\n","3  I went to see this film with a great deal of e...      -0.9707\n","4  \"Yes, I agree with everyone on this site this ...       0.3980\n","5  \"Jennifer Ehle was sparkling in \\\"\"Pride and P...       0.9494\n","6  Amy Poehler is a terrific comedian on Saturday...       0.8473\n","7  \"A plane carrying employees of a large biotech...       0.9864\n","8  A well made, gritty science fiction movie, it ...       0.9887\n","9  \"Incredibly dumb and utterly predictable story...      -0.7375"],"text/html":["\n","  <div id=\"df-a6f41700-a258-4f83-8520-5fee9a7d57a3\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>senti_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"Watching Time Chasers, it obvious that it was...</td>\n","      <td>-0.9095</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I saw this film about 20 years ago and remembe...</td>\n","      <td>-0.9694</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Minor Spoilers In New York, Joan Barnard (Elvi...</td>\n","      <td>-0.2794</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I went to see this film with a great deal of e...</td>\n","      <td>-0.9707</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"Yes, I agree with everyone on this site this ...</td>\n","      <td>0.3980</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>\"Jennifer Ehle was sparkling in \\\"\"Pride and P...</td>\n","      <td>0.9494</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Amy Poehler is a terrific comedian on Saturday...</td>\n","      <td>0.8473</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>\"A plane carrying employees of a large biotech...</td>\n","      <td>0.9864</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>A well made, gritty science fiction movie, it ...</td>\n","      <td>0.9887</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>\"Incredibly dumb and utterly predictable story...</td>\n","      <td>-0.7375</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6f41700-a258-4f83-8520-5fee9a7d57a3')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a6f41700-a258-4f83-8520-5fee9a7d57a3 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a6f41700-a258-4f83-8520-5fee9a7d57a3');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-fac5993b-7737-4fea-a932-6cfff693e2bc\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fac5993b-7737-4fea-a932-6cfff693e2bc')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-fac5993b-7737-4fea-a932-6cfff693e2bc button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["## chapter 6. 한국어 자연어 처리\n","### 띄어쓰기교정\n","* 한국어의 경우 띄어쓰기가 지켜지지 않아도 이해하기가 힘들지 않아, 오히려 띄어쓰기가 잘 안 지켜지는 경우가 많음.\n","* 띄어쓰기가 어긋난 데이터로 분석을 진행하면 전처리의 토큰화 과정부터 오류가 발생할 수 있어\n","* 전처리 단계에서 띄어쓰기를 미리 교정해야 함\n","* 무료 띄어쓰기 도구; py-hanspell (네이버 맞춤법 검사기를 이용)\n","    * colab에서 인스톨 한 후, spell_checker의 동작이 정상적이지 않을 수 있음\n","    * 이는, 네이버 맞춤법 검사기의 passportKey와 _callback 값이 매일 바뀌기 때문\n","    * passportKey와 _callback 값은 아래 방법으로 취득\n","        * 네이버 맞춤법 web 접근\n","        * F12 로 코드 확인\n","        * passportKey와 _callback을 검색\n","    * 그날 그날 올바른 passportKey와 _callback 값을, spell_checker.py (/usr/local/lib/python3.10/dist-packages/hanspell/spell_checkr.py) 의 payload 변수에 다음과 같이 변경해 주어야 함\n","    ```\n","    payload = {\n","            'passportKey': '737d31055a3116cb5b66f233997f2bb7f86d988b',\n","            '_callback': 'jQuery224020524192873615132_1701671014045',\n","            'color_blindness': '0',\n","            'q': text\n","        }\n","\n","    ```\n","    * **변경 후 반드시 런타임을 재실행해야함**\n","    \n","\n","### 형태소분석\n","* 단어의 어근과 접사를 분리\n","* 형태소 분석을 위한 많은 분석기가 공개되어 있고, 대표적인 한국어 형태소 분석기는 KoNLPy\n","* KoNLPy로, 문장분리, 형태소분석, 어간추출, 의미역추출, 개체명인식 등을 손쉽게 할 수 있음\n","* 그 외의 형태소 분석 도구들\n","    * [soynlp](https://github.com/lovit/soynlp); L tokenizer, MaxScoreTokenizer 등 다양한 형태소 분석기 제공\n","    * [Khaiii](https://tech.kakao.com/2018/12/13/khaiii/); 2018년 카카오가 공개한 오픈소스 한국어 형태소 분석기\n","    * [Google sentencepiece](https://github.com/google/sentencepiece); 2018년에 구굴에서 공개한 형태소 분석 패키지\n","\n","### 양질의데이터확보\n","* 사용인구가 다른 대표적인 언어보다 적고, 한국어의 독특한 특징때문에 전처리 작업이 까다로와 양질의 데이터 확보가 어려움\n","* 공개되어 있는 양질의 한국어 데이터\n","    * [KorQuAD](https://korquad.github.io/) (Korean Questions and Answers Dataset); 2018 LG CNS. 위키피디아와 전통적인 인쇄 및 전자 출판물 자료 기반. version2.0은 1.0보다 2만쌍 추가된 12만쌍\n","    * [네이버 영화 리뷰](https://github.com/e9t/nsmc/); 감성 지수에 대한 레이블이 함께 기록. 한국어 감성 분석용. 20만개 데이터중 10만개가 긍정, 10만개가 부정\n","    * [한국어 위키](https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EB%8C%80%EB%AC%B8); 현재 한국어 코퍼스 중 가장 많은 양의 데이터를 보유. 다운로드할 수 있게 되어 있지 않아서, 필요하면 크롤링을 해야 함\n"],"metadata":{"id":"prK6Dc5RHOo6"}},{"cell_type":"code","source":["!pip3 install --upgrade pip\n","#!pip install py-hanspell\n","# colab의 경우 아래와 같이 인스톨\n","#!pip install git+https://github.com/ssut/py-hanspell.git\n","\n","!pip3 install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qatCFVw8JnQ","executionInfo":{"status":"ok","timestamp":1701674763926,"user_tz":-540,"elapsed":10395,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"8479c8af-7561-4c46-b523-d98ae06b10a1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# chapter 6, lesson 1 띄어쓰기 교정\n","#import sys\n","#sys.path.append('/content/drive/MyDrive/Colab Notebooks/py-hanspell-master/')\n","#sys.path.append('/content/drive/MyDrive/Colab Notebooks/py-hanspell-master/hanspell/')\n","from hanspell import spell_checker\n","\n","text = \"아버지가방에들어가신다나는오늘코딩을했다\"\n","\n","hanspell_sent = spell_checker.check(text)\n","print(hanspell_sent.checked)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3_FeTnsIYPh","executionInfo":{"status":"ok","timestamp":1701672593938,"user_tz":-540,"elapsed":310,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"f0b1ab0d-bb2f-4019-a3dd-7886298c7e15"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["아버지가 방에 들어가신다 나는 오늘 코딩을 했다\n"]}]},{"cell_type":"code","source":["# chapter 6, lesson 2 형태소 분석\n","import konlpy\n","from konlpy.tag import Kkma, Komoran, Okt, Hannanum  # 형태소 분석기. 이 외에도 mecab가 유명\n","\n","kkma = Kkma()\n","komoran = Komoran()\n","okt = Okt()\n","hannanum = Hannanum()\n","\n","text = \"아버지가 방에 들어가신다 나는 오늘 코딩을 했다\"\n","\n","print(\"Kkma: \", kkma.morphs(text))\n","print(\"Komoran: \", komoran.morphs(text))\n","print(\"Okt: \", okt.morphs(text))\n","print(\"Hannanum: \", hannanum.morphs(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pm2WTZPC7-xp","executionInfo":{"status":"ok","timestamp":1701674838599,"user_tz":-540,"elapsed":5195,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"6ff77f49-5128-4c24-83fb-39067710d957"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Kkma:  ['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다', '나', '는', '오늘', '코딩', '을', '하', '었', '다']\n","Komoran:  ['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다', '나', '는', '오늘', '코', '딩', '을', '하', '았', '다']\n","Okt:  ['아버지', '가', '방', '에', '들어가신다', '나', '는', '오늘', '코딩', '을', '했다']\n","Hannanum:  ['아버지', '가', '방', '에', '들', '어', '가', '시ㄴ다', '나', '는', '오늘', '코딩', '을', '하', '었다']\n"]}]}]}