{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/magnetclip/nlp/blob/main/codeitnlp.ipynb","timestamp":1679026934887}],"mount_file_id":"1BnlcZ-HHZuPgzNzZlc1jFJK6_wYjGWdz","authorship_tag":"ABX9TyOdHnZE3ta/FgqWOShRu/mX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 자연어전처리과정\n","\n","* 단어 토큰화(word tokenization); 띄어쓰기, 문장기호(', , 등)을 기준으로 단어 리스트화\n","* 정제(cleaning); 코퍼스(말뭉치)에서 의미 없거나 목적에 접합하지 않은 단어를 제거 (예. 빈도수 2 이하인 단어, 길이가 2 이하인 단어 등)\n","* 불용어(stopwords) 정의; 의미 없거나 목적에서 벗어나는 단어(목적에 맞게 정의하여)를 제거 (예. do, then, wha, she, am, are ...)\n","* 정규화(normalization); 형태는 다르지만 같은 의미로 사용되는 단어를 하나로 통일 (예. US, USA, U.S., United States of America ...)\n","* 어간추출(stemming); 특정단어의 핵심이 되는 부분(어간)을 찾아 정규화(예. alize->al, ational->ate, ate->제거 ment->제거 등). porter stemmer, lancaster stemmer 등이 있음\n","* "],"metadata":{"id":"pZxLZU9MLh3Q"}},{"cell_type":"code","source":["# library import and function definition\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","nltk.download('averaged_perceptron_tagger')  # for tagger\n","from collections import Counter\n","from nltk.stem import PorterStemmer\n","from nltk.tag import pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet as wn\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","def clean_by_freq(tokenized_words, cut_off_count):\n","    vocab = Counter(tokenized_words)\n","\n","    uncommon_words = [key for key, value in vocab.items() if value <= cut_off_count]\n","    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n","\n","    return cleaned_words\n","\n","def clean_by_len(tokenized_words, cut_off_length):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        if len(word) > cut_off_length:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","def clean_by_stopwords(tokenized_words, stopwords_set):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        # 여기에 코드를 작성하세요\n","        if word not in stopwords_set:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","# 포터 스테머 어간 추출 함수\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        # porter_stemmed_words.append(porter_stemmer.stem(word))\n","        stem = porter_stemmer.stem(word)\n","        porter_stemmed_words.append(stem)\n","\n","    return porter_stemmed_words\n","\n","# 품사 태깅 함수\n","def pos_tagger(tokenized_sents):\n","    pos_tagged_words = []\n","    for sentence in tokenized_sents:\n","        # word tokenize\n","        tokenized_words = word_tokenize(sentence)\n","\n","        # pos\n","        pos_tagged = pos_tag(tokenized_words)\n","        pos_tagged_words.extend(pos_tagged)\n","    return pos_tagged_words\n","\n","# 품사 태깅 변환\n","def penn_to_wn(tag):\n","    if tag.startswith('J'):\n","        return wn.ADJ\n","    elif tag.startswith('N'):\n","        return wn.NOUN\n","    elif tag.startswith('R'):\n","        return wn.ADV\n","    elif tag.startswith('V'):\n","        return wn.VERB\n","    else:\n","        return\n","\n","# 표제어(lemmatization)\n","def word_lemmatizer(pos_tagged_words):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = []\n","    for word, tag in tagged_words:\n","        wn_tag = penn_to_wn(tag)\n","        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n","        else:\n","            lemmatized_words.append(word)\n","    return lemmatized_words\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7WT2tOhcfc1C","executionInfo":{"status":"ok","timestamp":1679441363655,"user_tz":-540,"elapsed":889,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"5ab712d2-690d-4d66-ab41-34e6889cd749"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hf1SV5ZKzrHG"},"outputs":[],"source":["from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","text = \"Although it's not a happily-ever-after ending, it is very realistic.\"\n","\n","# 단어 토큰화  https://www.nltk.org/api/nltk.tokenize.html\n","tokenized_words = word_tokenize(text)\n","\n","print(tokenized_words)"]},{"cell_type":"code","source":["import nltk\n","#from text import TEXT\n","from nltk.tokenize import word_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","\n","# 단어 토큰화\n","tokenized_words = word_tokenize(corpus)\n","\n","print(tokenized_words)"],"metadata":{"id":"icl3F9g62AkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","\n","corpus = TEXT\n","\n","# token list\n","tokenized_words = word_tokenize(corpus)\n","#print(tokenized_words)\n","\n","# token count\n","vocab = Counter(tokenized_words)\n","print(len(vocab))\n","\n","uncommon_words = [key for key, value in vocab.items() if value <= 2]\n","print('frequency <= 2;', len(uncommon_words))\n","\n","cleaned_by_freq = [word for word in tokenized_words if word not in uncommon_words]\n","print('frequency >= 3; ', len(cleaned_by_freq))"],"metadata":{"id":"Zp32mCwr246U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","#from text import TEXT\n","#nltk.download('punkt')\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_words = word_tokenize(corpus)\n","\n","def clean_by_freq(tokenized_words, cut_off_count):\n","    vocab = Counter(tokenized_words)\n","\n","    uncommon_words = [key for key, value in vocab.items() if value <= cut_off_count]\n","    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n","\n","    return cleaned_words\n","\n","def clean_by_len(tokenized_words, cut_off_length):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        if len(word) > cut_off_length:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","clean_by_freq = clean_by_freq(tokenized_words, 2)\n","cleaned_words = clean_by_len(clean_by_freq, 2)\n","\n","#cleaned_words"],"metadata":{"id":"QxoLBe1uG_7g","executionInfo":{"status":"ok","timestamp":1679273401165,"user_tz":-540,"elapsed":424,"user":{"displayName":"M J","userId":"09515631461713927918"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","#nltk.download('stopwords')\n","\n","stopwords_set = set(stopwords.words('english'))\n","\n","print('stopwords count :', len(stopwords_set))\n","#print(stopwords_set)\n","\n","stopwords_set.add('hello')\n","stopwords_set.remove('the')\n","stopwords_set.remove('me')\n","\n","#print('stopwords count is', len(stopwords_set))\n","#print('stopwords are', stopwords_set)\n","\n","cleaned_words = []\n","\n","for word in cleaned_by_freq:\n","    if word not in stopwords_set:\n","        cleaned_words.append(word)\n","\n","print('불용어 제거 전; ', len(cleaned_by_freq))\n","print('불용어 제거 후; ', len(cleaned_words))"],"metadata":{"id":"tLWmUukdCogn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리 07.불용어제거실습\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","#from text import TEXT\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","corpus = TEXT\n","tokenized_words = word_tokenize(TEXT)\n","\n","# NLTK에서 제공하는 불용어 목록을 세트 자료형으로 받아와 주세요\n","stopwords_set = set(stopwords.words('english'))\n","\n","def clean_by_stopwords(tokenized_words, stopwords_set):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        # 여기에 코드를 작성하세요\n","        if word not in stopwords_set:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","# 테스트 코드\n","#clean_by_stopwords(tokenized_words, stopwords_set)"],"metadata":{"id":"9jeoAkLAHKFK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679273426739,"user_tz":-540,"elapsed":303,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"47035c2d-c297-498e-fe38-a74c2269d868"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리  레슨 08.정규화\n","text = \"What can I do for you? Do your homework now.\"\n","print(text.lower()) # 대소문자 통합\n","\n","synonym_dict = {'US':'USA', 'U.S':'USA', 'Ummm':'Umm', 'Ummmm':'Umm'}\n","text = \"She became a US citizen. Ummmm, I think, maybe and or.\"\n","normalized_words = []\n","\n","tokenized_words = nltk.word_tokenize(text)\n","\n","for word in tokenized_words:\n","    if word in synonym_dict.keys():\n","        word = synonym_dict[word]\n","\n","    normalized_words.append(word)\n","print(normalized_words)"],"metadata":{"id":"L4dJ6vRKzMIU","executionInfo":{"status":"ok","timestamp":1679027851509,"user_tz":-540,"elapsed":590,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"ea8e5984-9f23-47cf-f22c-9a78d1bb6d7c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["what can i do for you? do your homework now.\n","['She', 'became', 'a', 'USA', 'citizen', '.', 'Umm', ',', 'I', 'think', ',', 'maybe', 'and', 'or', '.']\n"]}]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리 레슨 09.어간추출\n","from nltk.tokenize import word_tokenize\n","import nltk\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","from nltk.stem import PorterStemmer\n","from nltk.stem import LancasterStemmer\n","\n","porter_stemmer = PorterStemmer()\n","lancaster_stemmer = LancasterStemmer()\n","text = \"You are so lovely. I am loving you now.\"\n","porter_stemmed_words = []\n","lancaster_stemmed_words = []\n","\n","tokenized_words = nltk.word_tokenize(text)\n","\n","for word in tokenized_words:\n","    stem = porter_stemmer.stem(word)\n","    porter_stemmed_words.append(stem)\n","\n","for word in tokenized_words:\n","    stem = lancaster_stemmer.stem(word)\n","    lancaster_stemmed_words.append(stem)\n","\n","print('before; ', tokenized_words)\n","print('porter; ', porter_stemmed_words)\n","print('lancaster; ', lancaster_stemmed_words)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ukn6cI7oLHWH","executionInfo":{"status":"ok","timestamp":1679269471011,"user_tz":-540,"elapsed":362,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"7cdec36d-7f73-4c2a-89bb-8719622adb90"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["before;  ['You', 'are', 'so', 'lovely', '.', 'I', 'am', 'loving', 'you', 'now', '.']\n","porter;  ['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']\n","lancaster;  ['you', 'ar', 'so', 'lov', '.', 'i', 'am', 'lov', 'you', 'now', '.']\n"]}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","# 포터 스테머 어간 추출 함수\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        # porter_stemmed_words.append(porter_stemmer.stem(word))\n","        stem = porter_stemmer.stem(word)\n","        porter_stemmed_words.append(stem)\n","\n","    return porter_stemmed_words"],"metadata":{"id":"w2Yc9dCyQ6g8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 필요한 패키지와 함수 불러오기\n","import nltk\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","#from text import TEXT\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_words = word_tokenize(corpus)\n","\n","# 포터 스테머의 어간 추출\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        porter_stemmed_words.append(porter_stemmer.stem(word))\n","\n","    return porter_stemmed_words\n","\n","stemming_by_porter(tokenized_words)"],"metadata":{"id":"e4HlCT3wR_6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","#from preprocess import clean_by_freq\n","#from preprocess import clean_by_len\n","#from preprocess import clean_by_stopwords\n","\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","# normalization\n","df['review'] = df['review'].str.lower()  \n","\n","# tokenize\n","df['word_tokens'] = df['review'].apply(word_tokenize)  \n","\n","# cleaning\n","stopwords_set = set(stopwords.words('english'))\n","df['cleaned_tokens'] = df['word_tokens'].apply(lambda x: clean_by_freq(x, 1))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n","\n","# stemming\n","df['stemmed_tokens'] = df['cleaned_tokens'].apply(stemming_by_porter)\n","\n","df['stemmed_tokens'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2zlhyrTUZT8","executionInfo":{"status":"ok","timestamp":1679307421191,"user_tz":-540,"elapsed":296,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"feb55836-5633-454b-ffb7-ea03b6993b2f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","<ipython-input-11-f673e7f482ef>:13: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"execute_result","data":{"text/plain":["['one',\n"," 'film',\n"," 'said',\n"," 'realli',\n"," 'bad',\n"," 'movi',\n"," 'like',\n"," 'said',\n"," 'realli',\n"," 'bad',\n"," 'movi',\n"," 'bad',\n"," 'one',\n"," 'film',\n"," 'like']"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"iMQXll-xWl23","executionInfo":{"status":"ok","timestamp":1679307416891,"user_tz":-540,"elapsed":304,"user":{"displayName":"M J","userId":"09515631461713927918"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# sentence tokenization\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_sents = sent_tokenize(corpus)\n","\n","tokenized_sents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CuRZcVRPiATn","executionInfo":{"status":"ok","timestamp":1679307838389,"user_tz":-540,"elapsed":381,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"a4334d7d-181c-4b63-adbb-1b6eca6183ef"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["[\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\",\n"," 'So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.',\n"," \"There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n"," 'Oh dear!',\n"," \"I shall be late!'\",\n"," '(when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually\\xa0took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.',\n"," 'In another moment down went Alice after it, never once considering how in the world she was to get out again.',\n"," 'The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.',\n"," 'Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next.',\n"," 'First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs.',\n"," \"She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\"]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["from nltk.tag import pos_tag  # part of speech tagging\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","import nltk\n","#nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","\n","text = \"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\"Hey, let\\'s pool our money together and make a really bad movie!\\\" Or something like that.\"\n","pos_tagged_words = []\n","\n","tokenized_sents = sent_tokenize(text)\n","for sentence in tokenized_sents:\n","    # word tokenize\n","    tokenized_words = word_tokenize(sentence)\n","\n","    # pos\n","    pos_tagged = pos_tag(tokenized_words)\n","    pos_tagged_words.extend(pos_tagged)\n","\n","print(pos_tagged_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TQZRnJMijp0K","executionInfo":{"status":"ok","timestamp":1679396676277,"user_tz":-540,"elapsed":416,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"3a44a89a-0aca-41ff-d55c-761241d4f5ed"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"stream","name":"stdout","text":["[('Watching', 'VBG'), ('Time', 'NNP'), ('Chasers', 'NNPS'), (',', ','), ('it', 'PRP'), ('obvious', 'VBZ'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('made', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('friends', 'NNS'), ('.', '.'), ('Maybe', 'RB'), ('they', 'PRP'), ('were', 'VBD'), ('sitting', 'VBG'), ('around', 'IN'), ('one', 'CD'), ('day', 'NN'), ('in', 'IN'), ('film', 'NN'), ('school', 'NN'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Hey', 'NNP'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('pool', 'VB'), ('our', 'PRP$'), ('money', 'NN'), ('together', 'RB'), ('and', 'CC'), ('make', 'VB'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('!', '.'), (\"''\", \"''\"), ('Or', 'CC'), ('something', 'NN'), ('like', 'IN'), ('that', 'DT'), ('.', '.')]\n"]}]},{"cell_type":"code","source":["# chapter3, lesson5 표제어 추출 (Lemmatization); 표제어(lemma)란 사전적 어원. am, are, is -> be\n","\n","from nltk.tokenize import word_tokenize\n","import nltk\n","#nltk.download('punkt')\n","from nltk.tag import pos_tag  # Penn Treebank POS Tag\n","from nltk.corpus import wordnet as wn # WordNet POS Tag\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","text = 'You are the happiest person.'\n","tokenize_words = word_tokenize(text)\n","\n","tagged_words = pos_tag(tokenize_words)\n","print(tagged_words)\n","\n","def penn_to_wn(tag):\n","    if tag.startswith('J'):\n","        return wn.ADJ\n","    elif tag.startswith('N'):\n","        return wn.NOUN\n","    elif tag.startswith('R'):\n","        return wn.ADV\n","    elif tag.startswith('V'):\n","        return wn.VERB\n","    else:\n","        return\n","\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_words = []\n","\n","for word, tag in tagged_words:\n","    wn_tag = penn_to_wn(tag)\n","    if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","        lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n","    else:\n","        lemmatized_words.append(word)\n","\n","print('before; ', tokenize_words)\n","print('after: ', lemmatized_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvn1Y_TKfjyE","executionInfo":{"status":"ok","timestamp":1679442397174,"user_tz":-540,"elapsed":6,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"76c59cea-307f-4229-e245-8204f9f6b557"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[('You', 'PRP'), ('are', 'VBP'), ('the', 'DT'), ('happiest', 'JJS'), ('person', 'NN'), ('.', '.')]\n","before;  ['You', 'are', 'the', 'happiest', 'person', '.']\n","after:  ['You', 'be', 'the', 'happy', 'person', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wq59gynTnwFB"},"execution_count":null,"outputs":[]}]}