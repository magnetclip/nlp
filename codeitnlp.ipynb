{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/magnetclip/nlp/blob/main/codeitnlp.ipynb","timestamp":1679026934887}],"mount_file_id":"1BnlcZ-HHZuPgzNzZlc1jFJK6_wYjGWdz","authorship_tag":"ABX9TyNHIZBdTV9/5FxuKDA2579a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 자연어전처리과정\n","\n","## 단어 단위의 전처리\n","* 단어 토큰화(word tokenization); 띄어쓰기, 문장기호(', , 등)을 기준으로 단어 리스트화\n","* 정제(cleaning); 코퍼스(말뭉치)에서 의미 없거나 목적에 접합하지 않은 단어를 제거 (예. 빈도수 2 이하인 단어, 길이가 2 이하인 단어 등)\n","* 불용어(stopwords) 정의; 의미 없거나 목적에서 벗어나는 단어(목적에 맞게 정의하여)를 제거 (예. do, then, wha, she, am, are ...)\n","* 정규화(normalization); 형태는 다르지만 같은 의미로 사용되는 단어를 하나로 통일 (예. US, USA, U.S., United States of America ...)\n","* 어간추출(stemming); 특정단어의 핵심이 되는 부분(어간)을 찾아 정규화(예. alize->al, ational->ate, ate->제거 ment->제거 등). porter stemmer, lancaster stemmer 등이 있음\n","\n","## 문장 단위의 전처리\n","* 문장 토큰화(sentence tokenization); 코퍼스를 문장 단위로 토큰화. 마침표를 기준으로 토큰화. dr. mr. 의 마침표는 문장으로 인식하지 않아야 함\n","* 품사 태깅(POS; part of speech tagging); 문장 안에서의 단어의 품사를 태깅\n","* 표제어추출(Lemmatization); 단어의 사전적 어원 태깅 (예. happyiest->happy, am, are, is->be)\n"," "],"metadata":{"id":"pZxLZU9MLh3Q"}},{"cell_type":"code","source":["# library import and function definition. preprocess.py\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","nltk.download('averaged_perceptron_tagger')  # for tagger\n","from collections import Counter\n","from nltk.stem import PorterStemmer\n","from nltk.tag import pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet as wn\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('sentiwordnet')\n","from nltk.corpus import sentiwordnet as swn\n","\n","stopwords_set = set(stopwords.words('english'))\n","\n","def clean_by_freq(tokenized_words, cut_off_count):\n","    vocab = Counter(tokenized_words)\n","\n","    uncommon_words = [key for key, value in vocab.items() if value <= cut_off_count]\n","    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n","\n","    return cleaned_words\n","\n","def clean_by_len(tokenized_words, cut_off_length):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        if len(word) > cut_off_length:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","def clean_by_stopwords(tokenized_words, stopwords_set):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        # 여기에 코드를 작성하세요\n","        if word not in stopwords_set:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","# 포터 스테머 어간 추출 함수\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        # porter_stemmed_words.append(porter_stemmer.stem(word))\n","        stem = porter_stemmer.stem(word)\n","        porter_stemmed_words.append(stem)\n","\n","    return porter_stemmed_words\n","\n","# 품사 태깅 함수\n","def pos_tagger(tokenized_sents):\n","    pos_tagged_words = []\n","    for sentence in tokenized_sents:\n","        # word tokenize\n","        tokenized_words = word_tokenize(sentence)\n","\n","        # pos\n","        pos_tagged = pos_tag(tokenized_words)\n","        pos_tagged_words.extend(pos_tagged)\n","    return pos_tagged_words\n","\n","# 품사 태깅 변환\n","def penn_to_wn(tag):\n","    if tag.startswith('J'):\n","        return wn.ADJ\n","    elif tag.startswith('N'):\n","        return wn.NOUN\n","    elif tag.startswith('R'):\n","        return wn.ADV\n","    elif tag.startswith('V'):\n","        return wn.VERB\n","    else:\n","        return\n","\n","# 표제어(lemmatization)\n","def word_lemmatizer(pos_tagged_words):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = []\n","    for word, tag in pos_tagged_words:\n","        wn_tag = penn_to_wn(tag)\n","        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n","        else:\n","            lemmatized_words.append(word)\n","    return lemmatized_words\n","\n","def combine(sentence):\n","    return ' '.join(sentence)\n","\n","def idx_encoder(tokens, word_to_idx):\n","    encoded_idx = []\n","    for token in tokens:\n","        idx = word_to_idx[token]\n","        encoded_idx.append(idx)\n","    return encoded_idx\n","\n","def swn_polarity(pos_tagged_words):  # 감성 지수를 구하는 코드\n","    senti_score = 0\n","\n","    for word, tag in pos_tagged_words:\n","        # PennTreeBank 기준 품사를 WordNet 기준 품사로 변경\n","        wn_tag = penn_to_wn(tag)\n","        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","            continue\n","    \n","        # Synset 확인, 어휘 사전에 없을 경우에는 스킵\n","        if not wn.synsets(word, wn_tag):\n","            continue\n","        else:\n","            synsets = wn.synsets(word, wn_tag)\n","    \n","        # SentiSynset 확인\n","        synset = synsets[0]\n","        swn_synset = swn.senti_synset(synset.name())\n","\n","        # 감성 지수 계산\n","        word_senti_score = (swn_synset.pos_score() - swn_synset.neg_score())\n","        senti_score += word_senti_score\n","\n","    return senti_score\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7WT2tOhcfc1C","executionInfo":{"status":"ok","timestamp":1680652278772,"user_tz":-540,"elapsed":2917,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"50f217ff-d1d8-4ec9-9941-cb50a7a2759c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hf1SV5ZKzrHG"},"outputs":[],"source":["from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","text = \"Although it's not a happily-ever-after ending, it is very realistic.\"\n","\n","# 단어 토큰화  https://www.nltk.org/api/nltk.tokenize.html\n","tokenized_words = word_tokenize(text)\n","\n","print(tokenized_words)"]},{"cell_type":"code","source":["import nltk\n","#from text import TEXT\n","from nltk.tokenize import word_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","\n","# 단어 토큰화\n","tokenized_words = word_tokenize(corpus)\n","\n","print(tokenized_words)"],"metadata":{"id":"icl3F9g62AkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","\n","corpus = TEXT\n","\n","# token list\n","tokenized_words = word_tokenize(corpus)\n","#print(tokenized_words)\n","\n","# token count\n","vocab = Counter(tokenized_words)\n","print(len(vocab))\n","\n","uncommon_words = [key for key, value in vocab.items() if value <= 2]\n","print('frequency <= 2;', len(uncommon_words))\n","\n","cleaned_by_freq = [word for word in tokenized_words if word not in uncommon_words]\n","print('frequency >= 3; ', len(cleaned_by_freq))"],"metadata":{"id":"Zp32mCwr246U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","#from text import TEXT\n","#nltk.download('punkt')\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_words = word_tokenize(corpus)\n","\n","def clean_by_freq(tokenized_words, cut_off_count):\n","    vocab = Counter(tokenized_words)\n","\n","    uncommon_words = [key for key, value in vocab.items() if value <= cut_off_count]\n","    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n","\n","    return cleaned_words\n","\n","def clean_by_len(tokenized_words, cut_off_length):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        if len(word) > cut_off_length:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","clean_by_freq = clean_by_freq(tokenized_words, 2)\n","cleaned_words = clean_by_len(clean_by_freq, 2)\n","\n","#cleaned_words"],"metadata":{"id":"QxoLBe1uG_7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","#nltk.download('stopwords')\n","\n","stopwords_set = set(stopwords.words('english'))\n","\n","print('stopwords count :', len(stopwords_set))\n","#print(stopwords_set)\n","\n","stopwords_set.add('hello')\n","stopwords_set.remove('the')\n","stopwords_set.remove('me')\n","\n","#print('stopwords count is', len(stopwords_set))\n","#print('stopwords are', stopwords_set)\n","\n","cleaned_words = []\n","\n","for word in cleaned_by_freq:\n","    if word not in stopwords_set:\n","        cleaned_words.append(word)\n","\n","print('불용어 제거 전; ', len(cleaned_by_freq))\n","print('불용어 제거 후; ', len(cleaned_words))"],"metadata":{"id":"tLWmUukdCogn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리 07.불용어제거실습\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","#from text import TEXT\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","corpus = TEXT\n","tokenized_words = word_tokenize(TEXT)\n","\n","# NLTK에서 제공하는 불용어 목록을 세트 자료형으로 받아와 주세요\n","stopwords_set = set(stopwords.words('english'))\n","\n","def clean_by_stopwords(tokenized_words, stopwords_set):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        # 여기에 코드를 작성하세요\n","        if word not in stopwords_set:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","# 테스트 코드\n","#clean_by_stopwords(tokenized_words, stopwords_set)"],"metadata":{"id":"9jeoAkLAHKFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리  레슨 08.정규화\n","text = \"What can I do for you? Do your homework now.\"\n","print(text.lower()) # 대소문자 통합\n","\n","synonym_dict = {'US':'USA', 'U.S':'USA', 'Ummm':'Umm', 'Ummmm':'Umm'}\n","text = \"She became a US citizen. Ummmm, I think, maybe and or.\"\n","normalized_words = []\n","\n","tokenized_words = nltk.word_tokenize(text)\n","\n","for word in tokenized_words:\n","    if word in synonym_dict.keys():\n","        word = synonym_dict[word]\n","\n","    normalized_words.append(word)\n","print(normalized_words)"],"metadata":{"id":"L4dJ6vRKzMIU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리 레슨 09.어간추출\n","from nltk.tokenize import word_tokenize\n","import nltk\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","from nltk.stem import PorterStemmer\n","from nltk.stem import LancasterStemmer\n","\n","porter_stemmer = PorterStemmer()\n","lancaster_stemmer = LancasterStemmer()\n","text = \"You are so lovely. I am loving you now.\"\n","porter_stemmed_words = []\n","lancaster_stemmed_words = []\n","\n","tokenized_words = nltk.word_tokenize(text)\n","\n","for word in tokenized_words:\n","    stem = porter_stemmer.stem(word)\n","    porter_stemmed_words.append(stem)\n","\n","for word in tokenized_words:\n","    stem = lancaster_stemmer.stem(word)\n","    lancaster_stemmed_words.append(stem)\n","\n","print('before; ', tokenized_words)\n","print('porter; ', porter_stemmed_words)\n","print('lancaster; ', lancaster_stemmed_words)\n","\n"],"metadata":{"id":"Ukn6cI7oLHWH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","# 포터 스테머 어간 추출 함수\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        # porter_stemmed_words.append(porter_stemmer.stem(word))\n","        stem = porter_stemmer.stem(word)\n","        porter_stemmed_words.append(stem)\n","\n","    return porter_stemmed_words"],"metadata":{"id":"w2Yc9dCyQ6g8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 필요한 패키지와 함수 불러오기\n","import nltk\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","#from text import TEXT\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_words = word_tokenize(corpus)\n","\n","# 포터 스테머의 어간 추출\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        porter_stemmed_words.append(porter_stemmer.stem(word))\n","\n","    return porter_stemmed_words\n","\n","stemming_by_porter(tokenized_words)"],"metadata":{"id":"e4HlCT3wR_6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","#from preprocess import clean_by_freq\n","#from preprocess import clean_by_len\n","#from preprocess import clean_by_stopwords\n","\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","# normalization\n","df['review'] = df['review'].str.lower()  \n","\n","# tokenize\n","df['word_tokens'] = df['review'].apply(word_tokenize)  \n","\n","# cleaning\n","stopwords_set = set(stopwords.words('english'))\n","df['cleaned_tokens'] = df['word_tokens'].apply(lambda x: clean_by_freq(x, 1))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n","\n","# stemming\n","df['stemmed_tokens'] = df['cleaned_tokens'].apply(stemming_by_porter)\n","\n","df['stemmed_tokens'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2zlhyrTUZT8","executionInfo":{"status":"ok","timestamp":1680097426645,"user_tz":-540,"elapsed":1929,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"e42b67b7-e6c8-42cd-8321-afcb9e2b1c1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","<ipython-input-3-f673e7f482ef>:13: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"execute_result","data":{"text/plain":["['one',\n"," 'film',\n"," 'said',\n"," 'realli',\n"," 'bad',\n"," 'movi',\n"," 'like',\n"," 'said',\n"," 'realli',\n"," 'bad',\n"," 'movi',\n"," 'bad',\n"," 'one',\n"," 'film',\n"," 'like']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"iMQXll-xWl23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sentence tokenization\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_sents = sent_tokenize(corpus)\n","\n","tokenized_sents"],"metadata":{"id":"CuRZcVRPiATn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.tag import pos_tag  # part of speech tagging\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","import nltk\n","#nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","\n","text = \"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\"Hey, let\\'s pool our money together and make a really bad movie!\\\" Or something like that.\"\n","pos_tagged_words = []\n","\n","tokenized_sents = sent_tokenize(text)\n","for sentence in tokenized_sents:\n","    # word tokenize\n","    tokenized_words = word_tokenize(sentence)\n","\n","    # pos\n","    pos_tagged = pos_tag(tokenized_words)\n","    pos_tagged_words.extend(pos_tagged)\n","\n","print(pos_tagged_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TQZRnJMijp0K","executionInfo":{"status":"ok","timestamp":1679396676277,"user_tz":-540,"elapsed":416,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"3a44a89a-0aca-41ff-d55c-761241d4f5ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"stream","name":"stdout","text":["[('Watching', 'VBG'), ('Time', 'NNP'), ('Chasers', 'NNPS'), (',', ','), ('it', 'PRP'), ('obvious', 'VBZ'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('made', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('friends', 'NNS'), ('.', '.'), ('Maybe', 'RB'), ('they', 'PRP'), ('were', 'VBD'), ('sitting', 'VBG'), ('around', 'IN'), ('one', 'CD'), ('day', 'NN'), ('in', 'IN'), ('film', 'NN'), ('school', 'NN'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Hey', 'NNP'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('pool', 'VB'), ('our', 'PRP$'), ('money', 'NN'), ('together', 'RB'), ('and', 'CC'), ('make', 'VB'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('!', '.'), (\"''\", \"''\"), ('Or', 'CC'), ('something', 'NN'), ('like', 'IN'), ('that', 'DT'), ('.', '.')]\n"]}]},{"cell_type":"code","source":["# chapter3, lesson5 표제어 추출 (Lemmatization); 표제어(lemma)란 사전적 어원. am, are, is -> be\n","\n","from nltk.tokenize import word_tokenize\n","import nltk\n","#nltk.download('punkt')\n","from nltk.tag import pos_tag  # Penn Treebank POS Tag\n","from nltk.corpus import wordnet as wn # WordNet POS Tag\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","text = 'You are the happiest person.'\n","tokenize_words = word_tokenize(text)\n","\n","tagged_words = pos_tag(tokenize_words)\n","print(tagged_words)\n","\n","def penn_to_wn(tag):\n","    if tag.startswith('J'):\n","        return wn.ADJ\n","    elif tag.startswith('N'):\n","        return wn.NOUN\n","    elif tag.startswith('R'):\n","        return wn.ADV\n","    elif tag.startswith('V'):\n","        return wn.VERB\n","    else:\n","        return\n","\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_words = []\n","\n","for word, tag in tagged_words:\n","    wn_tag = penn_to_wn(tag)\n","    if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","        lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n","    else:\n","        lemmatized_words.append(word)\n","\n","print('before; ', tokenize_words)\n","print('after; ', lemmatized_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvn1Y_TKfjyE","executionInfo":{"status":"ok","timestamp":1679442397174,"user_tz":-540,"elapsed":6,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"76c59cea-307f-4229-e245-8204f9f6b557"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('You', 'PRP'), ('are', 'VBP'), ('the', 'DT'), ('happiest', 'JJS'), ('person', 'NN'), ('.', '.')]\n","before;  ['You', 'are', 'the', 'happiest', 'person', '.']\n","after:  ['You', 'be', 'the', 'happy', 'person', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}]},{"cell_type":"code","source":["# nlp preprocessing II\n","\n","import pandas as pd\n","#import nltk\n","#from nltk.tokenize import word_tokenize\n","#from nltk.tokenize import sent_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","#from nltk.corpus import stopwords\n","#nltk.download('stopwords')\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n","\n","# sentence tokenization\n","df['review'] = df['review'].str.lower()\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)\n","#df['sent_tokens'][0]\n","\n","# 품사 태깅 pos_tagging\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n","print(df['pos_tagged_tokens'][0])\n","\n","# 표제어 추출 Lemmatization\n","df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(word_lemmatizer)\n","print(df['lemmatized_tokens'][0])\n","\n","# \n","stopwords_set = set(stopwords.words('english'))\n","df['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x: clean_by_freq(x, 1))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n","\n","# combination\n","df['combined_corpus'] = df['cleaned_tokens'].apply(combine)\n","df[['combined_corpus']]"],"metadata":{"id":"wq59gynTnwFB","colab":{"base_uri":"https://localhost:8080/","height":454},"executionInfo":{"status":"ok","timestamp":1680097579279,"user_tz":-540,"elapsed":3304,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"b27db4ff-4163-4af8-bcae-a30b41c8e009"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-1133de3f2b03>:11: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n"]},{"output_type":"stream","name":"stdout","text":["[('``', '``'), ('watching', 'JJ'), ('time', 'NN'), ('chasers', 'NNS'), (',', ','), ('it', 'PRP'), ('obvious', 'VBZ'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('made', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('friends', 'NNS'), ('.', '.'), ('maybe', 'RB'), ('they', 'PRP'), ('were', 'VBD'), ('sitting', 'VBG'), ('around', 'IN'), ('one', 'CD'), ('day', 'NN'), ('in', 'IN'), ('film', 'NN'), ('school', 'NN'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('\\\\', 'FW'), (\"''\", \"''\"), (\"''\", \"''\"), ('hey', 'NN'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('pool', 'VB'), ('our', 'PRP$'), ('money', 'NN'), ('together', 'RB'), ('and', 'CC'), ('make', 'VB'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('!', '.'), ('\\\\', 'NN'), (\"''\", \"''\"), (\"''\", \"''\"), ('or', 'CC'), ('something', 'NN'), ('like', 'IN'), ('that', 'DT'), ('.', '.'), ('what', 'WP'), ('ever', 'RB'), ('they', 'PRP'), ('said', 'VBD'), (',', ','), ('they', 'PRP'), ('still', 'RB'), ('ended', 'VBD'), ('up', 'RP'), ('making', 'VBG'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('--', ':'), ('dull', 'JJ'), ('story', 'NN'), (',', ','), ('bad', 'JJ'), ('script', 'NN'), (',', ','), ('lame', 'NN'), ('acting', 'NN'), (',', ','), ('poor', 'JJ'), ('cinematography', 'NN'), (',', ','), ('bottom', 'NN'), ('of', 'IN'), ('the', 'DT'), ('barrel', 'NN'), ('stock', 'NN'), ('music', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.'), ('all', 'DT'), ('corners', 'NNS'), ('were', 'VBD'), ('cut', 'VBN'), (',', ','), ('except', 'IN'), ('the', 'DT'), ('one', 'NN'), ('that', 'WDT'), ('would', 'MD'), ('have', 'VB'), ('prevented', 'VBN'), ('this', 'DT'), ('film', 'NN'), (\"'s\", 'POS'), ('release', 'NN'), ('.', '.'), ('life', 'NN'), (\"'s\", 'POS'), ('like', 'IN'), ('that', 'DT'), ('.', '.'), (\"''\", \"''\")]\n","['``', 'watching', 'time', 'chaser', ',', 'it', 'obvious', 'that', 'it', 'be', 'make', 'by', 'a', 'bunch', 'of', 'friend', '.', 'maybe', 'they', 'be', 'sit', 'around', 'one', 'day', 'in', 'film', 'school', 'and', 'say', ',', '\\\\', \"''\", \"''\", 'hey', ',', 'let', \"'s\", 'pool', 'our', 'money', 'together', 'and', 'make', 'a', 'really', 'bad', 'movie', '!', '\\\\', \"''\", \"''\", 'or', 'something', 'like', 'that', '.', 'what', 'ever', 'they', 'say', ',', 'they', 'still', 'end', 'up', 'make', 'a', 'really', 'bad', 'movie', '--', 'dull', 'story', ',', 'bad', 'script', ',', 'lame', 'acting', ',', 'poor', 'cinematography', ',', 'bottom', 'of', 'the', 'barrel', 'stock', 'music', ',', 'etc', '.', 'all', 'corner', 'be', 'cut', ',', 'except', 'the', 'one', 'that', 'would', 'have', 'prevent', 'this', 'film', \"'s\", 'release', '.', 'life', \"'s\", 'like', 'that', '.', \"''\"]\n"]},{"output_type":"execute_result","data":{"text/plain":["                                     combined_corpus\n","0  make one film say make really bad movie like s...\n","1                                          film film\n","2  new york joan barnard elvire audrey barnard jo...\n","3  film film jump send n't jump radio n't send re...\n","4  site movie bad even movie movie make movie spe...\n","5  ehle northam wonderful wonderful ehle northam ...\n","6  role movie n't author book funny author author...\n","7  plane ceo search rescue mission call ceo harla...\n","8  gritty movie movie keep sci-fi good keep suspe...\n","9                                          girl girl"],"text/html":["\n","  <div id=\"df-fb38534f-80f6-4bf3-8abc-51835d918ec2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>combined_corpus</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>make one film say make really bad movie like s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>film film</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>new york joan barnard elvire audrey barnard jo...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>film film jump send n't jump radio n't send re...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>site movie bad even movie movie make movie spe...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ehle northam wonderful wonderful ehle northam ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>role movie n't author book funny author author...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>plane ceo search rescue mission call ceo harla...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>gritty movie movie keep sci-fi good keep suspe...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>girl girl</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb38534f-80f6-4bf3-8abc-51835d918ec2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fb38534f-80f6-4bf3-8abc-51835d918ec2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fb38534f-80f6-4bf3-8abc-51835d918ec2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Test APPLY function\n","# apply를 하면, 데이터 구조를 따로 고려하지 않고도, 리스트의 엔티티별로 해당 함수를 적용해줌\n","\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n","tokens = []\n","\n","df['review'] = df['review'].str.lower()\n","df['review'][0]\n","for i in range(0, 10):\n","    # print(sent_tokenize(df['review'][i]))\n","    tokens.append(sent_tokenize(df['review'][i]))\n","\n","print(tokens[5])\n","#df['sent_tokens'] = sent_tokenize(df['review'][0])\n","#df['sent_tokens']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VHPAYz3w-lU0","executionInfo":{"status":"ok","timestamp":1679875434733,"user_tz":-540,"elapsed":1457,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"750f478d-b3b2-436d-c883-cd68f536b606"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","<ipython-input-2-9965a7ced788>:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n"]},{"output_type":"stream","name":"stdout","text":["['\"jennifer ehle was sparkling in \\\\\"\"pride and prejudice.\\\\\"\" jeremy northam was simply wonderful in \\\\\"\"the winslow boy.\\\\\"\" with actors of this caliber, this film had to have a lot going for it.', 'even those who were critical of the movie spoke of the wonderful sequences involving these two.', 'i was eager to see it.', 'it is with bitter disappointment, however, that i must report that this flick is a piece of trash.', 'the scenes between ehle and northam had no depth or tenderness or real passion; they consisted of hackneyed and unsubtle latter-day cinematic lust--voracious open-mouthed kissing and soft-porn humping.', \"lust can be entertaining if it's done with originality; this was tasteless and awful.\", 'ehle and northam have sullied their craft; they should be ashamed.', 'as for the modern part of the romance, i was unnerved by the effeminate appearance of the male lead.', \"aren't there any masculine men left in hollywood?\", 'the plot was kind of interesting; with a better script and a more imaginative director, it might have worked.', '1/10\"']\n"]}]},{"cell_type":"code","source":["# 정수 인코딩 Integer Encoding; 텍스트를 숫자데이터로 변환하는 방법. 토큰화된 각 단어에 특정 정수를 매핑\n","\n","#tokens = df['cleaned_tokens'][4]\n","tokens = sum(df['cleaned_tokens'], [])\n","\n","vocab = Counter(tokens)\n","vocab = vocab.most_common()\n","\n","word_to_idx = {}\n","i = 0\n","\n","for (word, frequency) in vocab:\n","    i += 1  # 0은 아무 의미 없는 (무시되는) 정수를 위해 남겨두고, 1부터 시작\n","    word_to_idx[word] = i\n","\n","print(word_to_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHSv1Qm2Xzsh","executionInfo":{"status":"ok","timestamp":1679877377956,"user_tz":-540,"elapsed":420,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"205b5d73-1df9-4542-ef4c-c0c42b430042"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'movie': 1, 'film': 2, \"n't\": 3, 'scene': 4, 'bad': 5, 'time': 6, 'reason': 7, 'make': 8, 'jim': 9, 'good': 10, 'one': 11, 'like': 12, 'could': 13, \"'re\": 14, 'quastel': 15, 'really': 16, 'even': 17, 'monster': 18, 'joan': 19, 'love': 20, 'author': 21, 'try': 22, 'dialogue': 23, 'idea': 24, 'italy': 25, 'colleague': 26, 'maggot': 27, 'end': 28, 'watch': 29, 'jump': 30, 'radio': 31, 'stand-up': 32, 'day': 33, 'terrible': 34, 'ehle': 35, 'northam': 36, 'search': 37, 'rescue': 38, 'call': 39, 'knowles': 40, 'henriksen': 41, 'easily': 42, 'see': 43, 'appear': 44, 'get': 45, 'character': 46, 'think': 47, 'use': 48, 'whether': 49, 'need': 50, 'though': 51, 'sci-fi': 52, 'look': 53, 'say': 54, 'new': 55, 'york': 56, 'barnard': 57, 'elvire': 58, 'audrey': 59, 'john': 60, 'saxon': 61, 'etruscan': 62, 'tomb': 63, 'drug': 64, 'story': 65, 'romantic': 66, 'waste': 67, 'etrusco': 68, 'send': 69, 'reporter': 70, 'fear': 71, 'site': 72, 'special': 73, 'describe': 74, 'actor': 75, 'stand': 76, 'comedian': 77, 'wonderful': 78, 'lust': 79, 'role': 80, 'book': 81, 'funny': 82, 'queen': 83, 'corn': 84, 'plane': 85, 'ceo': 86, 'mission': 87, 'harlan': 88, 'lance': 89, 'put': 90, 'wood': 91, 'two': 92, 'decent': 93, 'sasquatch': 94, 'edit': 95, 'want': 96, 'potential': 97, 'material': 98, 'relate': 99, 'crib': 100, 'exposition': 101, 'far': 102, 'costume': 103, 'would': 104, 'stereotype': 105, 'well': 106, 'effective': 107, 'occur': 108, 'line': 109, 'back': 110, 'irrelevant': 111, 'comment': 112, 'cut': 113, 'random': 114, 'show': 115, 'important': 116, 'either': 117, 'never': 118, 'leave': 119, 'gritty': 120, 'keep': 121, 'suspense': 122, 'girl': 123}\n"]}]},{"cell_type":"code","source":["def idx_encoder(tokens, word_to_idx):\n","    encoded_idx = []\n","    for token in tokens:\n","        idx = word_to_idx[token]\n","        encoded_idx.append(idx)\n","    return encoded_idx\n","\n","df['integer_encoded'] = df['cleaned_tokens'].apply(lambda x: idx_encoder(x, word_to_idx))\n","#print(df[['integer_encoded']])\n","\n","# padding to make a matrix with tokens x max_length\n","max_len = max(len(item) for item in df['integer_encoded'])\n","print(max_len)\n","\n","for tokens in df['integer_encoded']:\n","    while len(tokens) < max_len:\n","        tokens.append(0)\n","\n","df[['integer_encoded']]"],"metadata":{"id":"OAjhKYOCZX9X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","word_to_idx = {} # 단어 별 인덱스 부여하기 위한 딕셔너리\n","i = 0\n","encoded_idx = [] # 각 토큰의 정수 인덱스를 부여하기 위한 리스트\n","corpus = TEXT\n","\n","tokenized_words = word_tokenize(corpus)\n","\n","# 단어의 빈도수를 계산하여 정렬하는 코드를 작성하세요\n","vocab = Counter(tokenized_words)\n","vocab = vocab.most_common()\n","\n","for (word, frequency) in vocab:\n","    # 여기에 코드를 작성하세요\n","    i += 1\n","    word_to_idx[word] = i\n","\n","for word in tokenized_words:\n","    # 여기에 코드를 작성하세요\n","    idx = word_to_idx[word]\n","    encoded_idx.append(idx)\n","\n","# 테스트 코드\n","encoded_idx"],"metadata":{"id":"xMuFgsuHbwuO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 감성 분석\n","\n","### 종류\n","* 규칙 기반 감성 분석; 감성 어휘 사전을 기준으로 단어의 긍부정을 분류\n","* 머신러닝 기반 감성 분석; 다수의 코퍼스들을 통해 긍정단어와 부정단어를 구분하는 모델을 학습시켜 그 모델을 기반으로 감성지수를 확인\n","\n","### 어휘사전 (nltk.corpus)\n","WordNet/SentiWordNet은 NLTK에서 제공하는 대규모 영어 어휘 사전\n","* WordNet/Synset\n","    * 단어, 품사, 순번\n","* SentiWordNet/SentiSynset (0~1사이의 값. 긍정-부정으로 판단)\n","    * 긍정지수 pos_score, 부정지수 neg_score, 객관성지수 obj_score\n"],"metadata":{"id":"ZGdJ9jPJkKe8"}},{"cell_type":"code","source":["from nltk.corpus import sentiwordnet as swn\n","\n","#word = 'happy'\n","word = 'hard'\n","print(\"wordnet-{}: \".format(word), wn.synsets(word))\n","print(\"sentiwordnet-{}: \".format(word), list(swn.senti_synsets(word)))\n","# happy의 긍정, 부정, 중립 지수 확인하기\n","\n","word_sentisynsets = list(swn.senti_synsets(word))\n","\n","pos_score = happy_sentisynsets[0].pos_score()\n","neg_score = happy_sentisynsets[0].neg_score()\n","obj_score = happy_sentisynsets[0].obj_score()\n","#print(pos_score, neg_score, obj_score)\n","#print(pos_score - neg_score)\n","\n","# 품사 별 감성 지수 비교\n","adj_synsets = wn.synsets(word, wn.ADJ)\n","print('adj_synsets of {} is ...\\n'.format(word), adj_synsets)\n","adv_synsets = wn.synsets(word, wn.ADV)\n","print('adv_synsets of {} is ...\\n'.format(word), adv_synsets)\n","\n","adj_synset = adj_synsets[0]\n","adv_synset = adv_synsets[0]\n","\n","adj_senti_synset = swn.senti_synset(adj_synset.name())\n","adv_senti_synset = swn.senti_synset(adv_synset.name())\n","print(adj_senti_synset, adv_senti_synset)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_LzlVV0sN4h","executionInfo":{"status":"ok","timestamp":1680657745222,"user_tz":-540,"elapsed":378,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"48fa6725-918d-4c13-aa24-3ca5dbc6666a"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["wordnet-hard:  [Synset('difficult.a.01'), Synset('hard.a.02'), Synset('hard.a.03'), Synset('hard.s.04'), Synset('arduous.s.01'), Synset('unvoiced.a.01'), Synset('hard.a.07'), Synset('hard.a.08'), Synset('intemperate.s.03'), Synset('hard.s.10'), Synset('hard.s.11'), Synset('hard.s.12'), Synset('hard.r.01'), Synset('hard.r.02'), Synset('hard.r.03'), Synset('hard.r.04'), Synset('hard.r.05'), Synset('heavily.r.07'), Synset('hard.r.07'), Synset('hard.r.08'), Synset('hard.r.09'), Synset('hard.r.10')]\n","sentiwordnet-hard:  [SentiSynset('difficult.a.01'), SentiSynset('hard.a.02'), SentiSynset('hard.a.03'), SentiSynset('hard.s.04'), SentiSynset('arduous.s.01'), SentiSynset('unvoiced.a.01'), SentiSynset('hard.a.07'), SentiSynset('hard.a.08'), SentiSynset('intemperate.s.03'), SentiSynset('hard.s.10'), SentiSynset('hard.s.11'), SentiSynset('hard.s.12'), SentiSynset('hard.r.01'), SentiSynset('hard.r.02'), SentiSynset('hard.r.03'), SentiSynset('hard.r.04'), SentiSynset('hard.r.05'), SentiSynset('heavily.r.07'), SentiSynset('hard.r.07'), SentiSynset('hard.r.08'), SentiSynset('hard.r.09'), SentiSynset('hard.r.10')]\n","adj_synsets of hard is ...\n"," [Synset('difficult.a.01'), Synset('hard.a.02'), Synset('hard.a.03'), Synset('hard.s.04'), Synset('arduous.s.01'), Synset('unvoiced.a.01'), Synset('hard.a.07'), Synset('hard.a.08'), Synset('intemperate.s.03'), Synset('hard.s.10'), Synset('hard.s.11'), Synset('hard.s.12')]\n","adv_synsets of hard is ...\n"," [Synset('hard.r.01'), Synset('hard.r.02'), Synset('hard.r.03'), Synset('hard.r.04'), Synset('hard.r.05'), Synset('heavily.r.07'), Synset('hard.r.07'), Synset('hard.r.08'), Synset('hard.r.09'), Synset('hard.r.10')]\n","<difficult.a.01: PosScore=0.0 NegScore=0.75> <hard.r.01: PosScore=0.125 NegScore=0.125>\n"]}]},{"cell_type":"code","source":["from nltk.corpus import sentiwordnet as swn\n","word = 'love'\n","pos = wn.VERB\n","\n","word_synsets = wn.synsets(word, pos)\n","\n","word_synset = word_synsets[0]\n","word_senti_synset = swn.senti_synset(word_synset.name())\n","\n","pos_score = word_senti_synset.pos_score()\n","neg_score = word_senti_synset.neg_score()\n","\n","sentiment_score = pos_score - neg_score\n","print(sentiment_score)\n","\n","# 또는 -------- \n","word_sentisynsets = list(swn.senti_synsets(word, pos))\n","pos_score = word_sentisynsets[0].pos_score()\n","neg_score = word_sentisynsets[0].neg_score()\n","print(pos_score-neg_score)\n","# --------\n","\n","#word_synsets =  wn.synsets(word, pos)\n","#word_synset = word_synsets[0]\n","#print(word_synset)\n","#word_senti_synset = swn.senti_synset(word_synsets[0].name())\n","#word_senti_synset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_1h4tUW108I","executionInfo":{"status":"ok","timestamp":1680002031682,"user_tz":-540,"elapsed":7,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"2a3ecf71-cbd5-453c-8ab1-fc3fc9d421a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.5\n","0.5\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","#print('df is \\n', df)\n","\n","# sentence tokenization; sentence 별로 분리하는 작업\n","df['review'] = df['review'].str.lower()\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)\n","print(\"df['sent_tokens'] is \\n\", df['sent_tokens'])\n","\n","# 품사 태깅 pos_tagging\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n","print(\"df['pos_tagged_tokens'] is \\n\", df['pos_tagged_tokens'])\n","\n","# 표제어 추출 Lemmatization\n","df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(word_lemmatizer)\n","#print(df['lemmatized_tokens'][0])\n","\n","# \n","stopwords_set = set(stopwords.words('english'))\n","df['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x: clean_by_freq(x, 1))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n","\n","pos_tagged_words = df['pos_tagged_tokens'][0]\n","senti_score = 0\n","\n","for word, tag in pos_tagged_words:\n","    wn_tag = penn_to_wn(tag)\n","    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","        continue\n","\n","    if not wn.synsets(word, wn_tag):\n","        continue\n","    else: \n","        synsets = wn.synsets(word, wn_tag)\n","\n","    synset = synsets[0]\n","    # print(synset.name())\n","    swn_synset = swn.senti_synset(synset.name())\n","\n","    word_senti_score = (swn_synset.pos_score() - swn_synset.neg_score()) \n","    senti_score += word_senti_score\n","\n","print(senti_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVlhu9ydnKBH","executionInfo":{"status":"ok","timestamp":1680135573579,"user_tz":-540,"elapsed":763,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"d137dc50-c9b7-4ee2-afb7-9aed68e1e75c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-52a3715171b9>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"stream","name":"stdout","text":["df['sent_tokens'] is \n"," 0    [\"watching time chasers, it obvious that it wa...\n","1    [i saw this film about 20 years ago and rememb...\n","2    [minor spoilers in new york, joan barnard (elv...\n","3    [i went to see this film with a great deal of ...\n","4    [\"yes, i agree with everyone on this site this...\n","5    [\"jennifer ehle was sparkling in \\\"\"pride and ...\n","6    [amy poehler is a terrific comedian on saturda...\n","7    [\"a plane carrying employees of a large biotec...\n","8    [a well made, gritty science fiction movie, it...\n","9    [\"incredibly dumb and utterly predictable stor...\n","Name: sent_tokens, dtype: object\n","df['pos_tagged_tokens'] is \n"," 0    [(``, ``), (watching, JJ), (time, NN), (chaser...\n","1    [(i, NN), (saw, VBD), (this, DT), (film, NN), ...\n","2    [(minor, JJ), (spoilers, NNS), (in, IN), (new,...\n","3    [(i, JJ), (went, VBD), (to, TO), (see, VB), (t...\n","4    [(``, ``), (yes, RB), (,, ,), (i, JJ), (agree,...\n","5    [(``, ``), (jennifer, NN), (ehle, NN), (was, V...\n","6    [(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...\n","7    [(``, ``), (a, DT), (plane, NN), (carrying, VB...\n","8    [(a, DT), (well, NN), (made, VBN), (,, ,), (gr...\n","9    [(``, ``), (incredibly, RB), (dumb, JJ), (and,...\n","Name: pos_tagged_tokens, dtype: object\n","-0.375\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","print('df is \\n', df)\n","index = 8\n","\n","# sentence tokenization; sentence 별로 분리하는 작업\n","df['review'] = df['review'].str.lower()\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)\n","print(\"df['sent_tokens'] is \\n\", df['sent_tokens'][index])\n","\n","# 문장 안에서의 품사 태깅 pos_tagging (pos ; part of speech)\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n","#print(\"df['pos_tagged_tokens'] is \\n\", df['pos_tagged_tokens'])\n","\n","df['swn_sentiment'] = df['pos_tagged_tokens'].apply(swn_polarity)\n","print(df.iloc[index][['review', 'swn_sentiment']])\n","\n","df['review'][index]\n","#swn_polarity['review'][1]"],"metadata":{"id":"09hZLhtxvNXG","colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"status":"ok","timestamp":1680653711241,"user_tz":-540,"elapsed":923,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"1960ec6f-5828-44ac-fbf4-1ed38b54db3b"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-27-16a6d0255225>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"stream","name":"stdout","text":["df is \n","                                               review\n","0  \"Watching Time Chasers, it obvious that it was...\n","1  I saw this film about 20 years ago and remembe...\n","2  Minor Spoilers In New York, Joan Barnard (Elvi...\n","3  I went to see this film with a great deal of e...\n","4  \"Yes, I agree with everyone on this site this ...\n","5  \"Jennifer Ehle was sparkling in \\\"\"Pride and P...\n","6  Amy Poehler is a terrific comedian on Saturday...\n","7  \"A plane carrying employees of a large biotech...\n","8  A well made, gritty science fiction movie, it ...\n","9  \"Incredibly dumb and utterly predictable story...\n","df['sent_tokens'] is \n"," ['a well made, gritty science fiction movie, it could be lost among hundreds of other similar movies, but it has several strong points to keep it near the top.', 'for one, the writing and directing is very solid, and it manages for the most part to avoid many sci-fi cliches, though not all of them.', 'it does a good job of keeping you in suspense, and the landscape and look of the movie will appeal to sci-fi fans.', \"if you're looking for a masterpiece, this isn't it.\", \"but if you're looking for good old fashioned post-apoc, gritty future in space sci-fi, with good suspense and special effects, then this is the movie for you.\", 'thoroughly enjoyable, and a good ending.']\n","review           a well made, gritty science fiction movie, it ...\n","swn_sentiment                                                  4.5\n","Name: 8, dtype: object\n"]},{"output_type":"execute_result","data":{"text/plain":["\"a well made, gritty science fiction movie, it could be lost among hundreds of other similar movies, but it has several strong points to keep it near the top. for one, the writing and directing is very solid, and it manages for the most part to avoid many sci-fi cliches, though not all of them. it does a good job of keeping you in suspense, and the landscape and look of the movie will appeal to sci-fi fans. if you're looking for a masterpiece, this isn't it. but if you're looking for good old fashioned post-apoc, gritty future in space sci-fi, with good suspense and special effects, then this is the movie for you. thoroughly enjoyable, and a good ending.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","# import pos_tagger, penn_to_wn from preprocess\n","from nltk.corpus import wordnet as wn\n","from nltk.corpus import sentiwordnet as swn\n","# download nltk.download('punkt', 'wordnet', 'sentiwordnet', averaged_perception_tagger')\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)  # tokenize sentense\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)  # tag 품사 to part of speech\n","df['pos_tagged_tokens']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O2c0svKrxVWd","executionInfo":{"status":"ok","timestamp":1680654061051,"user_tz":-540,"elapsed":541,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"43dc076d-29ad-41b8-8b32-d53dd4848774"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-32-f47e11ca0174>:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"execute_result","data":{"text/plain":["0    [(``, ``), (Watching, JJ), (Time, NN), (Chaser...\n","1    [(I, PRP), (saw, VBD), (this, DT), (film, NN),...\n","2    [(Minor, JJ), (Spoilers, NNS), (In, IN), (New,...\n","3    [(I, PRP), (went, VBD), (to, TO), (see, VB), (...\n","4    [(``, ``), (Yes, UH), (,, ,), (I, PRP), (agree...\n","5    [(``, ``), (Jennifer, NNP), (Ehle, NNP), (was,...\n","6    [(Amy, NNP), (Poehler, NNP), (is, VBZ), (a, DT...\n","7    [(``, ``), (A, DT), (plane, NN), (carrying, VB...\n","8    [(A, DT), (well, NN), (made, VBN), (,, ,), (gr...\n","9    [(``, ``), (Incredibly, RB), (dumb, JJ), (and,...\n","Name: pos_tagged_tokens, dtype: object"]},"metadata":{},"execution_count":32}]}]}