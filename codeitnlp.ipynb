{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/magnetclip/nlp/blob/main/codeitnlp.ipynb","timestamp":1679026934887}],"mount_file_id":"1BnlcZ-HHZuPgzNzZlc1jFJK6_wYjGWdz","authorship_tag":"ABX9TyNKVRtJVUNjGuMGHZ9xlaH4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 자연어전처리과정\n","\n","## 단어 단위의 전처리\n","* 단어 토큰화(word tokenization); 띄어쓰기, 문장기호(', , 등)을 기준으로 단어 리스트화\n","* 정제(cleaning); 코퍼스(분석에 활용하기 위한 자연어 데이터 (말뭉치))에서 의미 없거나 목적에 접합하지 않은 단어를 제거 (예. 빈도수 2 이하인 단어, 길이가 2 이하인 단어 등)\n","* 불용어(stopwords) 정의; 의미 없거나 목적에서 벗어나는 단어(목적에 맞게 정의하여)를 제거 (예. do, then, wha, she, am, are ...)\n","* 정규화(normalization); 형태는 다르지만 같은 의미로 사용되는 단어를 하나로 통일 (예. US, USA, U.S., United States of America ...)\n","* 어간추출(stemming); 특정단어의 핵심이 되는 부분(어간)을 찾아 정규화(예. alize->al, ational->ate, ate->제거 ment->제거 등). porter stemmer, lancaster stemmer 등이 있음\n","\n","## 문장 단위의 전처리\n","* 문장 토큰화(sentence tokenization); 코퍼스를 문장 단위로 토큰화. 마침표를 기준으로 토큰화. dr. mr. 의 마침표는 문장으로 인식하지 않아야 함\n","* 품사 태깅(POS; part of speech tagging); 문장 안에서의 단어의 품사를 태깅\n","* 표제어추출(Lemmatization); 단어의 사전적 어원 태깅 (예. happyiest->happy, am, are, is->be)\n",""],"metadata":{"id":"pZxLZU9MLh3Q"}},{"cell_type":"code","source":["# library import and function definition. preprocess.py\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","nltk.download('averaged_perceptron_tagger')  # for tagger\n","from collections import Counter\n","from nltk.stem import PorterStemmer\n","from nltk.tag import pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet as wn\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('sentiwordnet')\n","from nltk.corpus import sentiwordnet as swn\n","#\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","\n","\n","stopwords_set = set(stopwords.words('english'))\n","\n","def clean_by_freq(tokenized_words, cut_off_count):\n","    vocab = Counter(tokenized_words)\n","\n","    uncommon_words = [key for key, value in vocab.items() if value <= cut_off_count]\n","    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n","\n","    return cleaned_words\n","\n","def clean_by_len(tokenized_words, cut_off_length):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        if len(word) > cut_off_length:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","def clean_by_stopwords(tokenized_words, stopwords_set):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        # 여기에 코드를 작성하세요\n","        if word not in stopwords_set:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","# 포터 스테머 어간 추출 함수\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        # porter_stemmed_words.append(porter_stemmer.stem(word))\n","        stem = porter_stemmer.stem(word)\n","        porter_stemmed_words.append(stem)\n","\n","    return porter_stemmed_words\n","\n","# 품사 태깅 함수\n","def pos_tagger(tokenized_sents):\n","    pos_tagged_words = []\n","    for sentence in tokenized_sents:\n","        # word tokenize\n","        tokenized_words = word_tokenize(sentence)\n","\n","        # pos\n","        pos_tagged = pos_tag(tokenized_words)\n","        pos_tagged_words.extend(pos_tagged)\n","    return pos_tagged_words\n","\n","# 품사 태깅 변환\n","def penn_to_wn(tag):\n","    if tag.startswith('J'):\n","        return wn.ADJ\n","    elif tag.startswith('N'):\n","        return wn.NOUN\n","    elif tag.startswith('R'):\n","        return wn.ADV\n","    elif tag.startswith('V'):\n","        return wn.VERB\n","    else:\n","        return\n","\n","# 표제어(lemmatization)\n","def word_lemmatizer(pos_tagged_words):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = []\n","    for word, tag in pos_tagged_words:\n","        wn_tag = penn_to_wn(tag)\n","        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n","        else:\n","            lemmatized_words.append(word)\n","    return lemmatized_words\n","\n","def combine(sentence):\n","    return ' '.join(sentence)\n","\n","def idx_encoder(tokens, word_to_idx):\n","    encoded_idx = []\n","    for token in tokens:\n","        idx = word_to_idx[token]\n","        encoded_idx.append(idx)\n","    return encoded_idx\n","\n","def swn_polarity(pos_tagged_words):  # 감성 지수를 구하는 코드\n","    senti_score = 0\n","\n","    for word, tag in pos_tagged_words:\n","        # PennTreeBank 기준 품사를 WordNet 기준 품사로 변경\n","        wn_tag = penn_to_wn(tag)\n","        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","            continue\n","\n","        # Synset 확인, 어휘 사전에 없을 경우에는 스킵\n","        if not wn.synsets(word, wn_tag):\n","            continue\n","        else:\n","            synsets = wn.synsets(word, wn_tag)\n","\n","        # SentiSynset 확인\n","        synset = synsets[0]\n","        swn_synset = swn.senti_synset(synset.name())\n","\n","        # 감성 지수 계산\n","        word_senti_score = (swn_synset.pos_score() - swn_synset.neg_score())\n","        senti_score += word_senti_score\n","\n","    return senti_score\n","\n","def vader_sentiment(text):\n","    analyzer = SentimentIntensityAnalyzer()\n","\n","    senti_score = analyzer.polarity_scores(text)['compound']\n","\n","    return senti_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7WT2tOhcfc1C","executionInfo":{"status":"ok","timestamp":1680763064806,"user_tz":-540,"elapsed":371,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"9a980742-6fdc-44e7-94a6-7358d64d5b7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n","[nltk_data]   Package sentiwordnet is already up-to-date!\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hf1SV5ZKzrHG"},"outputs":[],"source":["from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","text = \"Although it's not a happily-ever-after ending, it is very realistic.\"\n","\n","# 단어 토큰화  https://www.nltk.org/api/nltk.tokenize.html\n","tokenized_words = word_tokenize(text)\n","\n","print(tokenized_words)"]},{"cell_type":"code","source":["import nltk\n","#from text import TEXT\n","from nltk.tokenize import word_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","\n","# 단어 토큰화\n","tokenized_words = word_tokenize(corpus)\n","\n","print(tokenized_words)"],"metadata":{"id":"icl3F9g62AkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","\n","corpus = TEXT\n","\n","# token list\n","tokenized_words = word_tokenize(corpus)\n","#print(tokenized_words)\n","\n","# token count\n","vocab = Counter(tokenized_words)\n","print(len(vocab))\n","\n","uncommon_words = [key for key, value in vocab.items() if value <= 2]\n","print('frequency <= 2;', len(uncommon_words))\n","\n","cleaned_by_freq = [word for word in tokenized_words if word not in uncommon_words]\n","print('frequency >= 3; ', len(cleaned_by_freq))"],"metadata":{"id":"Zp32mCwr246U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","#from text import TEXT\n","#nltk.download('punkt')\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_words = word_tokenize(corpus)\n","\n","def clean_by_freq(tokenized_words, cut_off_count):\n","    vocab = Counter(tokenized_words)\n","\n","    uncommon_words = [key for key, value in vocab.items() if value <= cut_off_count]\n","    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n","\n","    return cleaned_words\n","\n","def clean_by_len(tokenized_words, cut_off_length):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        if len(word) > cut_off_length:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","clean_by_freq = clean_by_freq(tokenized_words, 2)\n","cleaned_words = clean_by_len(clean_by_freq, 2)\n","\n","#cleaned_words"],"metadata":{"id":"QxoLBe1uG_7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","#nltk.download('stopwords')\n","\n","stopwords_set = set(stopwords.words('english'))\n","\n","print('stopwords count :', len(stopwords_set))\n","#print(stopwords_set)\n","\n","stopwords_set.add('hello')\n","stopwords_set.remove('the')\n","stopwords_set.remove('me')\n","\n","#print('stopwords count is', len(stopwords_set))\n","#print('stopwords are', stopwords_set)\n","\n","cleaned_words = []\n","\n","for word in cleaned_by_freq:\n","    if word not in stopwords_set:\n","        cleaned_words.append(word)\n","\n","print('불용어 제거 전; ', len(cleaned_by_freq))\n","print('불용어 제거 후; ', len(cleaned_words))"],"metadata":{"id":"tLWmUukdCogn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리 07.불용어제거실습\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","#from text import TEXT\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","corpus = TEXT\n","tokenized_words = word_tokenize(TEXT)\n","\n","# NLTK에서 제공하는 불용어 목록을 세트 자료형으로 받아와 주세요\n","stopwords_set = set(stopwords.words('english'))\n","\n","def clean_by_stopwords(tokenized_words, stopwords_set):\n","    cleaned_words = []\n","\n","    for word in tokenized_words:\n","        # 여기에 코드를 작성하세요\n","        if word not in stopwords_set:\n","            cleaned_words.append(word)\n","\n","    return cleaned_words\n","\n","# 테스트 코드\n","#clean_by_stopwords(tokenized_words, stopwords_set)"],"metadata":{"id":"9jeoAkLAHKFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리  레슨 08.정규화\n","text = \"What can I do for you? Do your homework now.\"\n","print(text.lower()) # 대소문자 통합\n","\n","synonym_dict = {'US':'USA', 'U.S':'USA', 'Ummm':'Umm', 'Ummmm':'Umm'}\n","text = \"She became a US citizen. Ummmm, I think, maybe and or.\"\n","normalized_words = []\n","\n","tokenized_words = nltk.word_tokenize(text)\n","\n","for word in tokenized_words:\n","    if word in synonym_dict.keys():\n","        word = synonym_dict[word]\n","\n","    normalized_words.append(word)\n","print(normalized_words)"],"metadata":{"id":"L4dJ6vRKzMIU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 챕터 02.단어단위전처리 레슨 09.어간추출\n","from nltk.tokenize import word_tokenize\n","import nltk\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","from nltk.stem import PorterStemmer\n","from nltk.stem import LancasterStemmer\n","\n","porter_stemmer = PorterStemmer()\n","lancaster_stemmer = LancasterStemmer()\n","text = \"You are so lovely. I am loving you now.\"\n","porter_stemmed_words = []\n","lancaster_stemmed_words = []\n","\n","tokenized_words = nltk.word_tokenize(text)\n","\n","for word in tokenized_words:\n","    stem = porter_stemmer.stem(word)\n","    porter_stemmed_words.append(stem)\n","\n","for word in tokenized_words:\n","    stem = lancaster_stemmer.stem(word)\n","    lancaster_stemmed_words.append(stem)\n","\n","print('before; ', tokenized_words)\n","print('porter; ', porter_stemmed_words)\n","print('lancaster; ', lancaster_stemmed_words)\n","\n"],"metadata":{"id":"Ukn6cI7oLHWH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","\n","# 포터 스테머 어간 추출 함수\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        # porter_stemmed_words.append(porter_stemmer.stem(word))\n","        stem = porter_stemmer.stem(word)\n","        porter_stemmed_words.append(stem)\n","\n","    return porter_stemmed_words"],"metadata":{"id":"w2Yc9dCyQ6g8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 필요한 패키지와 함수 불러오기\n","import nltk\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","#from text import TEXT\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_words = word_tokenize(corpus)\n","\n","# 포터 스테머의 어간 추출\n","def stemming_by_porter(tokenized_words):\n","    porter_stemmer = PorterStemmer()\n","    porter_stemmed_words = []\n","\n","    for word in tokenized_words:\n","        porter_stemmed_words.append(porter_stemmer.stem(word))\n","\n","    return porter_stemmed_words\n","\n","stemming_by_porter(tokenized_words)"],"metadata":{"id":"e4HlCT3wR_6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","#from preprocess import clean_by_freq\n","#from preprocess import clean_by_len\n","#from preprocess import clean_by_stopwords\n","\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","# normalization\n","df['review'] = df['review'].str.lower()\n","\n","# tokenize\n","df['word_tokens'] = df['review'].apply(word_tokenize)\n","\n","# cleaning\n","stopwords_set = set(stopwords.words('english'))\n","df['cleaned_tokens'] = df['word_tokens'].apply(lambda x: clean_by_freq(x, 1))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n","\n","# stemming\n","df['stemmed_tokens'] = df['cleaned_tokens'].apply(stemming_by_porter)\n","\n","df['stemmed_tokens'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2zlhyrTUZT8","executionInfo":{"status":"ok","timestamp":1680097426645,"user_tz":-540,"elapsed":1929,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"e42b67b7-e6c8-42cd-8321-afcb9e2b1c1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","<ipython-input-3-f673e7f482ef>:13: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"execute_result","data":{"text/plain":["['one',\n"," 'film',\n"," 'said',\n"," 'realli',\n"," 'bad',\n"," 'movi',\n"," 'like',\n"," 'said',\n"," 'realli',\n"," 'bad',\n"," 'movi',\n"," 'bad',\n"," 'one',\n"," 'film',\n"," 'like']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"iMQXll-xWl23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sentence tokenization\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","\n","TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","corpus = TEXT\n","tokenized_sents = sent_tokenize(corpus)\n","\n","tokenized_sents"],"metadata":{"id":"CuRZcVRPiATn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.tag import pos_tag  # part of speech tagging\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","import nltk\n","#nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","\n","text = \"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\"Hey, let\\'s pool our money together and make a really bad movie!\\\" Or something like that.\"\n","pos_tagged_words = []\n","\n","tokenized_sents = sent_tokenize(text)\n","for sentence in tokenized_sents:\n","    # word tokenize\n","    tokenized_words = word_tokenize(sentence)\n","\n","    # pos\n","    pos_tagged = pos_tag(tokenized_words)\n","    pos_tagged_words.extend(pos_tagged)\n","\n","print(pos_tagged_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TQZRnJMijp0K","executionInfo":{"status":"ok","timestamp":1679396676277,"user_tz":-540,"elapsed":416,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"3a44a89a-0aca-41ff-d55c-761241d4f5ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"stream","name":"stdout","text":["[('Watching', 'VBG'), ('Time', 'NNP'), ('Chasers', 'NNPS'), (',', ','), ('it', 'PRP'), ('obvious', 'VBZ'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('made', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('friends', 'NNS'), ('.', '.'), ('Maybe', 'RB'), ('they', 'PRP'), ('were', 'VBD'), ('sitting', 'VBG'), ('around', 'IN'), ('one', 'CD'), ('day', 'NN'), ('in', 'IN'), ('film', 'NN'), ('school', 'NN'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('``', '``'), ('Hey', 'NNP'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('pool', 'VB'), ('our', 'PRP$'), ('money', 'NN'), ('together', 'RB'), ('and', 'CC'), ('make', 'VB'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('!', '.'), (\"''\", \"''\"), ('Or', 'CC'), ('something', 'NN'), ('like', 'IN'), ('that', 'DT'), ('.', '.')]\n"]}]},{"cell_type":"code","source":["# chapter3, lesson5 표제어 추출 (Lemmatization); 표제어(lemma)란 사전적 어원. am, are, is -> be\n","\n","from nltk.tokenize import word_tokenize\n","import nltk\n","#nltk.download('punkt')\n","from nltk.tag import pos_tag  # Penn Treebank POS Tag\n","from nltk.corpus import wordnet as wn # WordNet POS Tag\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","text = 'You are the happiest person.'\n","tokenize_words = word_tokenize(text)\n","\n","tagged_words = pos_tag(tokenize_words)\n","print(tagged_words)\n","\n","def penn_to_wn(tag):\n","    if tag.startswith('J'):\n","        return wn.ADJ\n","    elif tag.startswith('N'):\n","        return wn.NOUN\n","    elif tag.startswith('R'):\n","        return wn.ADV\n","    elif tag.startswith('V'):\n","        return wn.VERB\n","    else:\n","        return\n","\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_words = []\n","\n","for word, tag in tagged_words:\n","    wn_tag = penn_to_wn(tag)\n","    if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","        lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n","    else:\n","        lemmatized_words.append(word)\n","\n","print('before; ', tokenize_words)\n","print('after; ', lemmatized_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvn1Y_TKfjyE","executionInfo":{"status":"ok","timestamp":1679442397174,"user_tz":-540,"elapsed":6,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"76c59cea-307f-4229-e245-8204f9f6b557"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('You', 'PRP'), ('are', 'VBP'), ('the', 'DT'), ('happiest', 'JJS'), ('person', 'NN'), ('.', '.')]\n","before;  ['You', 'are', 'the', 'happiest', 'person', '.']\n","after:  ['You', 'be', 'the', 'happy', 'person', '.']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}]},{"cell_type":"code","source":["# nlp preprocessing II\n","\n","import pandas as pd\n","#import nltk\n","#from nltk.tokenize import word_tokenize\n","#from nltk.tokenize import sent_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","#from nltk.corpus import stopwords\n","#nltk.download('stopwords')\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n","\n","# sentence tokenization\n","df['review'] = df['review'].str.lower()\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)\n","#df['sent_tokens'][0]\n","\n","# 품사 태깅 pos_tagging\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n","print(df['pos_tagged_tokens'][0])\n","\n","# 표제어 추출 Lemmatization\n","df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(word_lemmatizer)\n","print(df['lemmatized_tokens'][0])\n","\n","#\n","stopwords_set = set(stopwords.words('english'))\n","df['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x: clean_by_freq(x, 1))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n","\n","# combination\n","df['combined_corpus'] = df['cleaned_tokens'].apply(combine)\n","df[['combined_corpus']]"],"metadata":{"id":"wq59gynTnwFB","colab":{"base_uri":"https://localhost:8080/","height":454},"executionInfo":{"status":"ok","timestamp":1680097579279,"user_tz":-540,"elapsed":3304,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"b27db4ff-4163-4af8-bcae-a30b41c8e009"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-1133de3f2b03>:11: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n"]},{"output_type":"stream","name":"stdout","text":["[('``', '``'), ('watching', 'JJ'), ('time', 'NN'), ('chasers', 'NNS'), (',', ','), ('it', 'PRP'), ('obvious', 'VBZ'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('made', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('friends', 'NNS'), ('.', '.'), ('maybe', 'RB'), ('they', 'PRP'), ('were', 'VBD'), ('sitting', 'VBG'), ('around', 'IN'), ('one', 'CD'), ('day', 'NN'), ('in', 'IN'), ('film', 'NN'), ('school', 'NN'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('\\\\', 'FW'), (\"''\", \"''\"), (\"''\", \"''\"), ('hey', 'NN'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('pool', 'VB'), ('our', 'PRP$'), ('money', 'NN'), ('together', 'RB'), ('and', 'CC'), ('make', 'VB'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('!', '.'), ('\\\\', 'NN'), (\"''\", \"''\"), (\"''\", \"''\"), ('or', 'CC'), ('something', 'NN'), ('like', 'IN'), ('that', 'DT'), ('.', '.'), ('what', 'WP'), ('ever', 'RB'), ('they', 'PRP'), ('said', 'VBD'), (',', ','), ('they', 'PRP'), ('still', 'RB'), ('ended', 'VBD'), ('up', 'RP'), ('making', 'VBG'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('--', ':'), ('dull', 'JJ'), ('story', 'NN'), (',', ','), ('bad', 'JJ'), ('script', 'NN'), (',', ','), ('lame', 'NN'), ('acting', 'NN'), (',', ','), ('poor', 'JJ'), ('cinematography', 'NN'), (',', ','), ('bottom', 'NN'), ('of', 'IN'), ('the', 'DT'), ('barrel', 'NN'), ('stock', 'NN'), ('music', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.'), ('all', 'DT'), ('corners', 'NNS'), ('were', 'VBD'), ('cut', 'VBN'), (',', ','), ('except', 'IN'), ('the', 'DT'), ('one', 'NN'), ('that', 'WDT'), ('would', 'MD'), ('have', 'VB'), ('prevented', 'VBN'), ('this', 'DT'), ('film', 'NN'), (\"'s\", 'POS'), ('release', 'NN'), ('.', '.'), ('life', 'NN'), (\"'s\", 'POS'), ('like', 'IN'), ('that', 'DT'), ('.', '.'), (\"''\", \"''\")]\n","['``', 'watching', 'time', 'chaser', ',', 'it', 'obvious', 'that', 'it', 'be', 'make', 'by', 'a', 'bunch', 'of', 'friend', '.', 'maybe', 'they', 'be', 'sit', 'around', 'one', 'day', 'in', 'film', 'school', 'and', 'say', ',', '\\\\', \"''\", \"''\", 'hey', ',', 'let', \"'s\", 'pool', 'our', 'money', 'together', 'and', 'make', 'a', 'really', 'bad', 'movie', '!', '\\\\', \"''\", \"''\", 'or', 'something', 'like', 'that', '.', 'what', 'ever', 'they', 'say', ',', 'they', 'still', 'end', 'up', 'make', 'a', 'really', 'bad', 'movie', '--', 'dull', 'story', ',', 'bad', 'script', ',', 'lame', 'acting', ',', 'poor', 'cinematography', ',', 'bottom', 'of', 'the', 'barrel', 'stock', 'music', ',', 'etc', '.', 'all', 'corner', 'be', 'cut', ',', 'except', 'the', 'one', 'that', 'would', 'have', 'prevent', 'this', 'film', \"'s\", 'release', '.', 'life', \"'s\", 'like', 'that', '.', \"''\"]\n"]},{"output_type":"execute_result","data":{"text/plain":["                                     combined_corpus\n","0  make one film say make really bad movie like s...\n","1                                          film film\n","2  new york joan barnard elvire audrey barnard jo...\n","3  film film jump send n't jump radio n't send re...\n","4  site movie bad even movie movie make movie spe...\n","5  ehle northam wonderful wonderful ehle northam ...\n","6  role movie n't author book funny author author...\n","7  plane ceo search rescue mission call ceo harla...\n","8  gritty movie movie keep sci-fi good keep suspe...\n","9                                          girl girl"],"text/html":["\n","  <div id=\"df-fb38534f-80f6-4bf3-8abc-51835d918ec2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>combined_corpus</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>make one film say make really bad movie like s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>film film</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>new york joan barnard elvire audrey barnard jo...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>film film jump send n't jump radio n't send re...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>site movie bad even movie movie make movie spe...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ehle northam wonderful wonderful ehle northam ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>role movie n't author book funny author author...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>plane ceo search rescue mission call ceo harla...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>gritty movie movie keep sci-fi good keep suspe...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>girl girl</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb38534f-80f6-4bf3-8abc-51835d918ec2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fb38534f-80f6-4bf3-8abc-51835d918ec2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fb38534f-80f6-4bf3-8abc-51835d918ec2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Test APPLY function\n","# apply를 하면, 데이터 구조를 따로 고려하지 않고도, 리스트의 엔티티별로 해당 함수를 적용해줌\n","\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","#nltk.download('punkt')  # needed for acronym such as Mr. Dr. ...\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n","tokens = []\n","\n","df['review'] = df['review'].str.lower()\n","df['review'][0]\n","for i in range(0, 10):\n","    # print(sent_tokenize(df['review'][i]))\n","    tokens.append(sent_tokenize(df['review'][i]))\n","\n","print(tokens[5])\n","#df['sent_tokens'] = sent_tokenize(df['review'][0])\n","#df['sent_tokens']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VHPAYz3w-lU0","executionInfo":{"status":"ok","timestamp":1679875434733,"user_tz":-540,"elapsed":1457,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"750f478d-b3b2-436d-c883-cd68f536b606"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","<ipython-input-2-9965a7ced788>:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitdata/imdb.tsv', delimiter=\"\\\\t\")\n"]},{"output_type":"stream","name":"stdout","text":["['\"jennifer ehle was sparkling in \\\\\"\"pride and prejudice.\\\\\"\" jeremy northam was simply wonderful in \\\\\"\"the winslow boy.\\\\\"\" with actors of this caliber, this film had to have a lot going for it.', 'even those who were critical of the movie spoke of the wonderful sequences involving these two.', 'i was eager to see it.', 'it is with bitter disappointment, however, that i must report that this flick is a piece of trash.', 'the scenes between ehle and northam had no depth or tenderness or real passion; they consisted of hackneyed and unsubtle latter-day cinematic lust--voracious open-mouthed kissing and soft-porn humping.', \"lust can be entertaining if it's done with originality; this was tasteless and awful.\", 'ehle and northam have sullied their craft; they should be ashamed.', 'as for the modern part of the romance, i was unnerved by the effeminate appearance of the male lead.', \"aren't there any masculine men left in hollywood?\", 'the plot was kind of interesting; with a better script and a more imaginative director, it might have worked.', '1/10\"']\n"]}]},{"cell_type":"code","source":["# 정수 인코딩 Integer Encoding; 텍스트를 숫자데이터로 변환하는 방법. 토큰화된 각 단어에 특정 정수를 매핑\n","\n","#tokens = df['cleaned_tokens'][4]\n","tokens = sum(df['cleaned_tokens'], [])\n","\n","vocab = Counter(tokens)\n","vocab = vocab.most_common()\n","\n","word_to_idx = {}\n","i = 0\n","\n","for (word, frequency) in vocab:\n","    i += 1  # 0은 아무 의미 없는 (무시되는) 정수를 위해 남겨두고, 1부터 시작\n","    word_to_idx[word] = i\n","\n","print(word_to_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHSv1Qm2Xzsh","executionInfo":{"status":"ok","timestamp":1679877377956,"user_tz":-540,"elapsed":420,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"205b5d73-1df9-4542-ef4c-c0c42b430042"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'movie': 1, 'film': 2, \"n't\": 3, 'scene': 4, 'bad': 5, 'time': 6, 'reason': 7, 'make': 8, 'jim': 9, 'good': 10, 'one': 11, 'like': 12, 'could': 13, \"'re\": 14, 'quastel': 15, 'really': 16, 'even': 17, 'monster': 18, 'joan': 19, 'love': 20, 'author': 21, 'try': 22, 'dialogue': 23, 'idea': 24, 'italy': 25, 'colleague': 26, 'maggot': 27, 'end': 28, 'watch': 29, 'jump': 30, 'radio': 31, 'stand-up': 32, 'day': 33, 'terrible': 34, 'ehle': 35, 'northam': 36, 'search': 37, 'rescue': 38, 'call': 39, 'knowles': 40, 'henriksen': 41, 'easily': 42, 'see': 43, 'appear': 44, 'get': 45, 'character': 46, 'think': 47, 'use': 48, 'whether': 49, 'need': 50, 'though': 51, 'sci-fi': 52, 'look': 53, 'say': 54, 'new': 55, 'york': 56, 'barnard': 57, 'elvire': 58, 'audrey': 59, 'john': 60, 'saxon': 61, 'etruscan': 62, 'tomb': 63, 'drug': 64, 'story': 65, 'romantic': 66, 'waste': 67, 'etrusco': 68, 'send': 69, 'reporter': 70, 'fear': 71, 'site': 72, 'special': 73, 'describe': 74, 'actor': 75, 'stand': 76, 'comedian': 77, 'wonderful': 78, 'lust': 79, 'role': 80, 'book': 81, 'funny': 82, 'queen': 83, 'corn': 84, 'plane': 85, 'ceo': 86, 'mission': 87, 'harlan': 88, 'lance': 89, 'put': 90, 'wood': 91, 'two': 92, 'decent': 93, 'sasquatch': 94, 'edit': 95, 'want': 96, 'potential': 97, 'material': 98, 'relate': 99, 'crib': 100, 'exposition': 101, 'far': 102, 'costume': 103, 'would': 104, 'stereotype': 105, 'well': 106, 'effective': 107, 'occur': 108, 'line': 109, 'back': 110, 'irrelevant': 111, 'comment': 112, 'cut': 113, 'random': 114, 'show': 115, 'important': 116, 'either': 117, 'never': 118, 'leave': 119, 'gritty': 120, 'keep': 121, 'suspense': 122, 'girl': 123}\n"]}]},{"cell_type":"code","source":["def idx_encoder(tokens, word_to_idx):\n","    encoded_idx = []\n","    for token in tokens:\n","        idx = word_to_idx[token]\n","        encoded_idx.append(idx)\n","    return encoded_idx\n","\n","df['integer_encoded'] = df['cleaned_tokens'].apply(lambda x: idx_encoder(x, word_to_idx))\n","#print(df[['integer_encoded']])\n","\n","# padding to make a matrix with tokens x max_length\n","max_len = max(len(item) for item in df['integer_encoded'])\n","print(max_len)\n","\n","for tokens in df['integer_encoded']:\n","    while len(tokens) < max_len:\n","        tokens.append(0)\n","\n","df[['integer_encoded']]"],"metadata":{"id":"OAjhKYOCZX9X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TEXT = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n","So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n","There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n","In another moment down went Alice after it, never once considering how in the world she was to get out again.\n","The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\n","Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed; it was labelled 'ORANGE MARMALADE', but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody, so managed to put it into one of the cupboards as she fell past it.\n","\"\"\"\n","\n","word_to_idx = {} # 단어 별 인덱스 부여하기 위한 딕셔너리\n","i = 0\n","encoded_idx = [] # 각 토큰의 정수 인덱스를 부여하기 위한 리스트\n","corpus = TEXT\n","\n","tokenized_words = word_tokenize(corpus)\n","\n","# 단어의 빈도수를 계산하여 정렬하는 코드를 작성하세요\n","vocab = Counter(tokenized_words)\n","vocab = vocab.most_common()\n","\n","for (word, frequency) in vocab:\n","    # 여기에 코드를 작성하세요\n","    i += 1\n","    word_to_idx[word] = i\n","\n","for word in tokenized_words:\n","    # 여기에 코드를 작성하세요\n","    idx = word_to_idx[word]\n","    encoded_idx.append(idx)\n","\n","# 테스트 코드\n","encoded_idx"],"metadata":{"id":"xMuFgsuHbwuO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 감성 분석\n","\n","### 종류\n","* 규칙 기반 감성 분석; 감성 어휘 사전을 기준으로 단어의 긍부정을 분류\n","* 머신러닝 기반 감성 분석; 다수의 코퍼스들을 통해 긍정단어와 부정단어를 구분하는 모델을 학습시켜 그 모델을 기반으로 감성지수를 확인\n","\n","### 어휘사전 (nltk.corpus)\n","WordNet/SentiWordNet은 NLTK에서 제공하는 대규모 영어 어휘 사전\n","* WordNet/Synset\n","    * 단어, 품사, 순번\n","* SentiWordNet/SentiSynset (0~1사이의 값. 긍정-부정으로 판단)\n","    * 긍정지수 pos_score, 부정지수 neg_score, 객관성지수 obj_score\n","\n","### VADER(Valence Aware Dictionary and sEntiment Reasoner)\n","* 감성 분석을 위한 어휘 사전이자 알고리즘\n","* SentiWordNet과의 큰 차이점은 일반적인 감성 어휘 사전의 규칙 외에도 축약형과 기호 등을 고려해 감성 지수를 추출할 수 있다는 점\n","* 그래서 주로 축약형 표현이나 특수 문자가 많이 사용된 소셜 미디어 텍스트를 분석할 때 자주 사용됨\n"],"metadata":{"id":"ZGdJ9jPJkKe8"}},{"cell_type":"code","source":["from nltk.corpus import sentiwordnet as swn\n","\n","#word = 'happy'\n","word = 'hard'\n","print(\"wordnet-{}: \".format(word), wn.synsets(word))\n","print(\"sentiwordnet-{}: \".format(word), list(swn.senti_synsets(word)))\n","# happy의 긍정, 부정, 중립 지수 확인하기\n","\n","word_sentisynsets = list(swn.senti_synsets(word))\n","\n","pos_score = happy_sentisynsets[0].pos_score()\n","neg_score = happy_sentisynsets[0].neg_score()\n","obj_score = happy_sentisynsets[0].obj_score()\n","#print(pos_score, neg_score, obj_score)\n","#print(pos_score - neg_score)\n","\n","# 품사 별 감성 지수 비교\n","adj_synsets = wn.synsets(word, wn.ADJ)\n","print('adj_synsets of {} is ...\\n'.format(word), adj_synsets)\n","adv_synsets = wn.synsets(word, wn.ADV)\n","print('adv_synsets of {} is ...\\n'.format(word), adv_synsets)\n","\n","adj_synset = adj_synsets[0]\n","adv_synset = adv_synsets[0]\n","\n","adj_senti_synset = swn.senti_synset(adj_synset.name())\n","adv_senti_synset = swn.senti_synset(adv_synset.name())\n","print(adj_senti_synset, adv_senti_synset)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_LzlVV0sN4h","executionInfo":{"status":"ok","timestamp":1680657745222,"user_tz":-540,"elapsed":378,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"48fa6725-918d-4c13-aa24-3ca5dbc6666a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["wordnet-hard:  [Synset('difficult.a.01'), Synset('hard.a.02'), Synset('hard.a.03'), Synset('hard.s.04'), Synset('arduous.s.01'), Synset('unvoiced.a.01'), Synset('hard.a.07'), Synset('hard.a.08'), Synset('intemperate.s.03'), Synset('hard.s.10'), Synset('hard.s.11'), Synset('hard.s.12'), Synset('hard.r.01'), Synset('hard.r.02'), Synset('hard.r.03'), Synset('hard.r.04'), Synset('hard.r.05'), Synset('heavily.r.07'), Synset('hard.r.07'), Synset('hard.r.08'), Synset('hard.r.09'), Synset('hard.r.10')]\n","sentiwordnet-hard:  [SentiSynset('difficult.a.01'), SentiSynset('hard.a.02'), SentiSynset('hard.a.03'), SentiSynset('hard.s.04'), SentiSynset('arduous.s.01'), SentiSynset('unvoiced.a.01'), SentiSynset('hard.a.07'), SentiSynset('hard.a.08'), SentiSynset('intemperate.s.03'), SentiSynset('hard.s.10'), SentiSynset('hard.s.11'), SentiSynset('hard.s.12'), SentiSynset('hard.r.01'), SentiSynset('hard.r.02'), SentiSynset('hard.r.03'), SentiSynset('hard.r.04'), SentiSynset('hard.r.05'), SentiSynset('heavily.r.07'), SentiSynset('hard.r.07'), SentiSynset('hard.r.08'), SentiSynset('hard.r.09'), SentiSynset('hard.r.10')]\n","adj_synsets of hard is ...\n"," [Synset('difficult.a.01'), Synset('hard.a.02'), Synset('hard.a.03'), Synset('hard.s.04'), Synset('arduous.s.01'), Synset('unvoiced.a.01'), Synset('hard.a.07'), Synset('hard.a.08'), Synset('intemperate.s.03'), Synset('hard.s.10'), Synset('hard.s.11'), Synset('hard.s.12')]\n","adv_synsets of hard is ...\n"," [Synset('hard.r.01'), Synset('hard.r.02'), Synset('hard.r.03'), Synset('hard.r.04'), Synset('hard.r.05'), Synset('heavily.r.07'), Synset('hard.r.07'), Synset('hard.r.08'), Synset('hard.r.09'), Synset('hard.r.10')]\n","<difficult.a.01: PosScore=0.0 NegScore=0.75> <hard.r.01: PosScore=0.125 NegScore=0.125>\n"]}]},{"cell_type":"code","source":["from nltk.corpus import sentiwordnet as swn\n","word = 'love'\n","pos = wn.VERB\n","\n","word_synsets = wn.synsets(word, pos)\n","\n","word_synset = word_synsets[0]\n","word_senti_synset = swn.senti_synset(word_synset.name())\n","\n","pos_score = word_senti_synset.pos_score()\n","neg_score = word_senti_synset.neg_score()\n","\n","sentiment_score = pos_score - neg_score\n","print(sentiment_score)\n","\n","# 또는 --------\n","word_sentisynsets = list(swn.senti_synsets(word, pos))\n","pos_score = word_sentisynsets[0].pos_score()\n","neg_score = word_sentisynsets[0].neg_score()\n","print(pos_score-neg_score)\n","# --------\n","\n","#word_synsets =  wn.synsets(word, pos)\n","#word_synset = word_synsets[0]\n","#print(word_synset)\n","#word_senti_synset = swn.senti_synset(word_synsets[0].name())\n","#word_senti_synset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_1h4tUW108I","executionInfo":{"status":"ok","timestamp":1680002031682,"user_tz":-540,"elapsed":7,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"2a3ecf71-cbd5-453c-8ab1-fc3fc9d421a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.5\n","0.5\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","#print('df is \\n', df)\n","\n","# sentence tokenization; sentence 별로 분리하는 작업\n","df['review'] = df['review'].str.lower()\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)\n","print(\"df['sent_tokens'] is \\n\", df['sent_tokens'])\n","\n","# 품사 태깅 pos_tagging\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n","print(\"df['pos_tagged_tokens'] is \\n\", df['pos_tagged_tokens'])\n","\n","# 표제어 추출 Lemmatization\n","df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(word_lemmatizer)\n","#print(df['lemmatized_tokens'][0])\n","\n","#\n","stopwords_set = set(stopwords.words('english'))\n","df['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x: clean_by_freq(x, 1))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n","df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n","\n","pos_tagged_words = df['pos_tagged_tokens'][0]\n","senti_score = 0\n","\n","for word, tag in pos_tagged_words:\n","    wn_tag = penn_to_wn(tag)\n","    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n","        continue\n","\n","    if not wn.synsets(word, wn_tag):\n","        continue\n","    else:\n","        synsets = wn.synsets(word, wn_tag)\n","\n","    synset = synsets[0]\n","    # print(synset.name())\n","    swn_synset = swn.senti_synset(synset.name())\n","\n","    word_senti_score = (swn_synset.pos_score() - swn_synset.neg_score())\n","    senti_score += word_senti_score\n","\n","print(senti_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVlhu9ydnKBH","executionInfo":{"status":"ok","timestamp":1680135573579,"user_tz":-540,"elapsed":763,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"d137dc50-c9b7-4ee2-afb7-9aed68e1e75c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-52a3715171b9>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"stream","name":"stdout","text":["df['sent_tokens'] is \n"," 0    [\"watching time chasers, it obvious that it wa...\n","1    [i saw this film about 20 years ago and rememb...\n","2    [minor spoilers in new york, joan barnard (elv...\n","3    [i went to see this film with a great deal of ...\n","4    [\"yes, i agree with everyone on this site this...\n","5    [\"jennifer ehle was sparkling in \\\"\"pride and ...\n","6    [amy poehler is a terrific comedian on saturda...\n","7    [\"a plane carrying employees of a large biotec...\n","8    [a well made, gritty science fiction movie, it...\n","9    [\"incredibly dumb and utterly predictable stor...\n","Name: sent_tokens, dtype: object\n","df['pos_tagged_tokens'] is \n"," 0    [(``, ``), (watching, JJ), (time, NN), (chaser...\n","1    [(i, NN), (saw, VBD), (this, DT), (film, NN), ...\n","2    [(minor, JJ), (spoilers, NNS), (in, IN), (new,...\n","3    [(i, JJ), (went, VBD), (to, TO), (see, VB), (t...\n","4    [(``, ``), (yes, RB), (,, ,), (i, JJ), (agree,...\n","5    [(``, ``), (jennifer, NN), (ehle, NN), (was, V...\n","6    [(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...\n","7    [(``, ``), (a, DT), (plane, NN), (carrying, VB...\n","8    [(a, DT), (well, NN), (made, VBN), (,, ,), (gr...\n","9    [(``, ``), (incredibly, RB), (dumb, JJ), (and,...\n","Name: pos_tagged_tokens, dtype: object\n","-0.375\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","print('df is \\n', df)\n","index = 8\n","\n","# sentence tokenization; sentence 별로 분리하는 작업\n","df['review'] = df['review'].str.lower()\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)\n","print(\"df['sent_tokens'] is \\n\", df['sent_tokens'][index])\n","\n","# 문장 안에서의 품사 태깅 pos_tagging (pos ; part of speech)\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n","#print(\"df['pos_tagged_tokens'] is \\n\", df['pos_tagged_tokens'])\n","\n","df['swn_sentiment'] = df['pos_tagged_tokens'].apply(swn_polarity)\n","print(df.iloc[index][['review', 'swn_sentiment']])\n","\n","#df['review'][index]\n","print(\"df['review'] is \", df['review'])\n","#swn_polarity['review'][1]"],"metadata":{"id":"09hZLhtxvNXG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680761945864,"user_tz":-540,"elapsed":799,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"bf1e9543-f48c-4dc3-fcd4-c2960ca70903"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-ee377df99556>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"stream","name":"stdout","text":["df is \n","                                               review\n","0  \"Watching Time Chasers, it obvious that it was...\n","1  I saw this film about 20 years ago and remembe...\n","2  Minor Spoilers In New York, Joan Barnard (Elvi...\n","3  I went to see this film with a great deal of e...\n","4  \"Yes, I agree with everyone on this site this ...\n","5  \"Jennifer Ehle was sparkling in \\\"\"Pride and P...\n","6  Amy Poehler is a terrific comedian on Saturday...\n","7  \"A plane carrying employees of a large biotech...\n","8  A well made, gritty science fiction movie, it ...\n","9  \"Incredibly dumb and utterly predictable story...\n","df['sent_tokens'] is \n"," ['a well made, gritty science fiction movie, it could be lost among hundreds of other similar movies, but it has several strong points to keep it near the top.', 'for one, the writing and directing is very solid, and it manages for the most part to avoid many sci-fi cliches, though not all of them.', 'it does a good job of keeping you in suspense, and the landscape and look of the movie will appeal to sci-fi fans.', \"if you're looking for a masterpiece, this isn't it.\", \"but if you're looking for good old fashioned post-apoc, gritty future in space sci-fi, with good suspense and special effects, then this is the movie for you.\", 'thoroughly enjoyable, and a good ending.']\n","review           a well made, gritty science fiction movie, it ...\n","swn_sentiment                                                  4.5\n","Name: 8, dtype: object\n","df['review'] is  0    \"watching time chasers, it obvious that it was...\n","1    i saw this film about 20 years ago and remembe...\n","2    minor spoilers in new york, joan barnard (elvi...\n","3    i went to see this film with a great deal of e...\n","4    \"yes, i agree with everyone on this site this ...\n","5    \"jennifer ehle was sparkling in \\\"\"pride and p...\n","6    amy poehler is a terrific comedian on saturday...\n","7    \"a plane carrying employees of a large biotech...\n","8    a well made, gritty science fiction movie, it ...\n","9    \"incredibly dumb and utterly predictable story...\n","Name: review, dtype: object\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","# import pos_tagger, penn_to_wn from preprocess\n","from nltk.corpus import wordnet as wn\n","from nltk.corpus import sentiwordnet as swn\n","# download nltk.download('punkt', 'wordnet', 'sentiwordnet', averaged_perception_tagger')\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","df['sent_tokens'] = df['review'].apply(sent_tokenize)  # tokenize sentense\n","df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)  # tag 품사 to part of speech\n","#df['pos_tagged_tokens']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"O2c0svKrxVWd","executionInfo":{"status":"ok","timestamp":1680761821143,"user_tz":-540,"elapsed":285,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"1eb9f6e1-5e03-4d7f-d6bb-673aeb7b4f29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-9641686e04bf>:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]},{"output_type":"execute_result","data":{"text/plain":["                                              review\n","0  \"Watching Time Chasers, it obvious that it was...\n","1  I saw this film about 20 years ago and remembe...\n","2  Minor Spoilers In New York, Joan Barnard (Elvi...\n","3  I went to see this film with a great deal of e...\n","4  \"Yes, I agree with everyone on this site this ...\n","5  \"Jennifer Ehle was sparkling in \\\"\"Pride and P...\n","6  Amy Poehler is a terrific comedian on Saturday...\n","7  \"A plane carrying employees of a large biotech...\n","8  A well made, gritty science fiction movie, it ...\n","9  \"Incredibly dumb and utterly predictable story..."],"text/html":["\n","  <div id=\"df-c6230a62-1a50-4f68-b680-99148aa8a218\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"Watching Time Chasers, it obvious that it was...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I saw this film about 20 years ago and remembe...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Minor Spoilers In New York, Joan Barnard (Elvi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I went to see this film with a great deal of e...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"Yes, I agree with everyone on this site this ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>\"Jennifer Ehle was sparkling in \\\"\"Pride and P...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Amy Poehler is a terrific comedian on Saturday...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>\"A plane carrying employees of a large biotech...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>A well made, gritty science fiction movie, it ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>\"Incredibly dumb and utterly predictable story...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6230a62-1a50-4f68-b680-99148aa8a218')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c6230a62-1a50-4f68-b680-99148aa8a218 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c6230a62-1a50-4f68-b680-99148aa8a218');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["!pip install vaderSentiment"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2SUU9rUVpiQE","executionInfo":{"status":"ok","timestamp":1680752586358,"user_tz":-540,"elapsed":4202,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"3e8c1258-9699-43f1-d75a-6ae144594eb0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from vaderSentiment) (2.27.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (1.26.15)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","\n","senti_analyzer = SentimentIntensityAnalyzer()\n","\n","text1 = \"This is a great movie!\"\n","text2 = \"This is a terrible movie!\"\n","text3 = \"This movie was just okay.\"\n","\n","senti_scores_text1 = senti_analyzer.polarity_scores(text1)\n","senti_scores_text2 = senti_analyzer.polarity_scores(text2)\n","senti_scores_text3 = senti_analyzer.polarity_scores(text3)\n","\n","print(senti_scores_text1)\n","print(senti_scores_text2)\n","print(senti_scores_text3)\n","\n","def vader_sentiment(text):\n","    analyzer = SentimentIntensityAnalyzer()\n","\n","    senti_score = analyzer.polarity_scores(text)['compound']\n","\n","    return senti_score\n","\n","df['vader_sentiment'] = df['review'].apply(vader_sentiment)\n","df[['review', 'swn_sentiment', 'vader_sentiment']]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"id":"uDChKs4iswDb","executionInfo":{"status":"ok","timestamp":1680763356932,"user_tz":-540,"elapsed":406,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"d6f57c6f-ebed-4d58-b3d1-3a61f83a7b21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'neg': 0.0, 'neu': 0.406, 'pos': 0.594, 'compound': 0.6588}\n","{'neg': 0.531, 'neu': 0.469, 'pos': 0.0, 'compound': -0.5255}\n","{'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.2263}\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["                                              review  swn_sentiment  \\\n","0  \"watching time chasers, it obvious that it was...         -0.375   \n","1  i saw this film about 20 years ago and remembe...         -1.500   \n","2  minor spoilers in new york, joan barnard (elvi...         -2.250   \n","3  i went to see this film with a great deal of e...         -0.500   \n","4  \"yes, i agree with everyone on this site this ...          3.000   \n","5  \"jennifer ehle was sparkling in \\\"\"pride and p...          6.750   \n","6  amy poehler is a terrific comedian on saturday...          0.750   \n","7  \"a plane carrying employees of a large biotech...          8.750   \n","8  a well made, gritty science fiction movie, it ...          4.500   \n","9  \"incredibly dumb and utterly predictable story...         -1.125   \n","\n","   vader_sentiment  \n","0          -0.9095  \n","1          -0.9694  \n","2          -0.2794  \n","3          -0.9707  \n","4           0.8049  \n","5           0.9494  \n","6           0.8473  \n","7           0.9885  \n","8           0.9887  \n","9          -0.7375  "],"text/html":["\n","  <div id=\"df-6dc817e1-7f06-4c75-9683-99bed308b630\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>swn_sentiment</th>\n","      <th>vader_sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"watching time chasers, it obvious that it was...</td>\n","      <td>-0.375</td>\n","      <td>-0.9095</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i saw this film about 20 years ago and remembe...</td>\n","      <td>-1.500</td>\n","      <td>-0.9694</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>minor spoilers in new york, joan barnard (elvi...</td>\n","      <td>-2.250</td>\n","      <td>-0.2794</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i went to see this film with a great deal of e...</td>\n","      <td>-0.500</td>\n","      <td>-0.9707</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"yes, i agree with everyone on this site this ...</td>\n","      <td>3.000</td>\n","      <td>0.8049</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>\"jennifer ehle was sparkling in \\\"\"pride and p...</td>\n","      <td>6.750</td>\n","      <td>0.9494</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>amy poehler is a terrific comedian on saturday...</td>\n","      <td>0.750</td>\n","      <td>0.8473</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>\"a plane carrying employees of a large biotech...</td>\n","      <td>8.750</td>\n","      <td>0.9885</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>a well made, gritty science fiction movie, it ...</td>\n","      <td>4.500</td>\n","      <td>0.9887</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>\"incredibly dumb and utterly predictable story...</td>\n","      <td>-1.125</td>\n","      <td>-0.7375</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6dc817e1-7f06-4c75-9683-99bed308b630')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6dc817e1-7f06-4c75-9683-99bed308b630 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6dc817e1-7f06-4c75-9683-99bed308b630');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["import nltk\n","import pandas as pd\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')\n","\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n","analyzer = SentimentIntensityAnalyzer()\n","\n","def vader_sentiment(text):\n","    # 여기에 코드를 작성하세요\n","    senti_score = analyzer.polarity_scores(text)['compound']\n","    return senti_score\n","\n","df['senti_score'] = df['review'].apply(vader_sentiment)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKeRQeJPMrYI","executionInfo":{"status":"ok","timestamp":1680763755798,"user_tz":-540,"elapsed":326,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"50b8ad83-1427-444b-f896-63e4cddc15a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n","<ipython-input-14-e5564dc30dc7>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/codeitnlp/nlp/imdb.tsv', delimiter='\\\\t')\n"]}]},{"cell_type":"markdown","source":["# 한국어 자연어 처리\n","### 띄어쓰기교정\n","* 한국어의 경우 띄어쓰기가 지켜지지 않아도 이해하기가 힘들지 않아, 오히려 띄어쓰기가 잘 안 지켜지는 경우가 많음.\n","* 띄어쓰기가 어긋난 데이터로 분석을 진행하면 전처리의 토큰화 과정부터 오류가 발생할 수 있어\n","* 전처리 단계에서 띄어쓰기를 미리 교정해야 함\n","* 무료 띄어쓰기 도구; py-hanspell (네이버 맞춤법 검사기를 이용)\n","    * colab에선 인스톨은 어떻게든 했으나, spell_checker의 동작이 정상적이지 않음\n","\n","### 형태소분석\n","* 단어의 어근과 접사를 분리\n","* 형태소 분석을 위한 많은 분석기가 공개되어 있고, 대표적인 한국어 형태소 분석기는 KoNLPy\n","* KoNLPy로, 문장분리, 형태소분석, 어간추출, 의미역추출, 개체명인식 등을 손쉽게 할 수 있음\n","\n","### 양질의데이터확보\n","* 사용인구가 다른 대표적인 언어보다 적고, 한국어의 독특한 특징때문에 전처리 작업이 까다로와 양질의 데이터 확보가 어려움\n","* 공개되어 있는 양질의 한국어 데이터\n","    * KorQuAD (Korean Questions and Answers Dataset); 2018 LG CNS. 위키피디아와 전통적인 인쇄 및 전자 출판물 자료 기반. version2.0은 1.0보다 2만쌍 추가된 12만쌍\n","    * 네이버 영화 리뷰; 감성 지수에 대한 레이블이 함께 기록. 한국어 감성 분석용. 20만개 데이터중 10만개가 긍정, 10만개가 부정\n","    * 한국어 위키; 현재 한국어 코퍼스 중 가장 많은 양의 데이터를 보유. 다운로드할 수 있게 되어 있지 않아서, 필요하면 크롤링을 해야 함\n"],"metadata":{"id":"prK6Dc5RHOo6"}},{"cell_type":"code","source":["!pip3 install --upgrade pip\n","!pip3 install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qatCFVw8JnQ","executionInfo":{"status":"ok","timestamp":1681092866180,"user_tz":-540,"elapsed":11739,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"c8c5dd00-5c4d-4f00-f408-d7369b878b68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (22.0.4)\n","Collecting pip\n","  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 22.0.4\n","    Uninstalling pip-22.0.4:\n","      Successfully uninstalled pip-22.0.4\n","Successfully installed pip-23.0.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.22.4)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (4.9.2)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.4.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/py-hanspell-master/')\n","#sys.path.append('/content/drive/MyDrive/Colab Notebooks/py-hanspell-master/hanspell/')\n","\n","from hanspell import spell_checker\n","\n","text = \"아버지가방에들어가신다나는오늘코딩을했다\"\n","\n","hanspell_sent = spell_checker.check(text)\n","print(hanspell_sent.checked)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"-3_FeTnsIYPh","executionInfo":{"status":"error","timestamp":1681092869433,"user_tz":-540,"elapsed":305,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"5ddff493-144e-4455-9fb3-59652a5bdb29"},"execution_count":null,"outputs":[{"output_type":"error","ename":"JSONDecodeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-7f2011524e1b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"아버지가방에들어가신다나는오늘코딩을했다\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mhanspell_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspell_checker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhanspell_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab Notebooks/py-hanspell-master/hanspell/spell_checker.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mpassed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;31m#r = r.text[42:-2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m44\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"]}]},{"cell_type":"code","source":["import konlpy\n","from konlpy.tag import Kkma, Komoran, Okt, Hannanum\n","\n","kkma = Kkma()\n","komoran = Komoran()\n","okt = Okt()\n","hannanum = Hannanum()\n","\n","text = \"아버지가 방에 들어가신다 나는 오늘 코딩을 했다\"\n","\n","print(\"Kkma: \", kkma.morphs(text))\n","print(\"Komoran: \", komoran.morphs(text))\n","print(\"Okt: \", okt.morphs(text))\n","print(\"Hannanum: \", hannanum.morphs(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pm2WTZPC7-xp","executionInfo":{"status":"ok","timestamp":1681092982548,"user_tz":-540,"elapsed":33383,"user":{"displayName":"M J","userId":"09515631461713927918"}},"outputId":"a7595db0-32f8-4f84-dcb4-6df0a3256572"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Kkma:  ['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다', '나', '는', '오늘', '코딩', '을', '하', '었', '다']\n","Komoran:  ['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다', '나', '는', '오늘', '코', '딩', '을', '하', '았', '다']\n","Okt:  ['아버지', '가', '방', '에', '들어가신다', '나', '는', '오늘', '코딩', '을', '했다']\n","Hannanum:  ['아버지', '가', '방', '에', '들', '어', '가', '시ㄴ다', '나', '는', '오늘', '코딩', '을', '하', '었다']\n"]}]}]}